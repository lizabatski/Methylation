{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('test.for.torch.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dataset Class\n",
    "## Notice that the __init__ method contains an argument `apply_log10`, if you set it to True\n",
    "## you will apply a log10 to the raw counts. We can experiment with this\n",
    "class MethDataset(Dataset):\n",
    "    def __init__(self, sequence, histone, methylation, coords, apply_log10=False):\n",
    "        self.sequence = sequence\n",
    "        self.histone = histone\n",
    "        self.methylation = methylation\n",
    "        self.transform = apply_log10\n",
    "        self.coords = coords\n",
    "        self.histone_names = ['H3K4me3', 'H3K36me2', 'H3K27me3', 'H3K9me3']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.methylation.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sequence = torch.from_numpy(self.sequence[idx])\n",
    "        histone = self.histone.astype(np.float32)\n",
    "\n",
    "        H3K4me3 = torch.from_numpy(histone[:, :, 0][idx].astype(np.float32)) if not self.transform else torch.from_numpy(np.log10(histone[:, :, 0]+1e-4)[idx])\n",
    "        H3K36me2 = torch.from_numpy(histone[:, :, 1][idx].astype(np.float32)) if not self.transform else torch.from_numpy(np.log10(histone[:, :, 1]+1e-4)[idx])\n",
    "        H3K27me3 = torch.from_numpy(histone[:, :, 2][idx].astype(np.float32)) if not self.transform else torch.from_numpy(np.log10(histone[:, :, 2]+1e-4)[idx])\n",
    "        H3K9me3 = torch.from_numpy(histone[:, :, 3][idx].astype(np.float32)) if not self.transform else torch.from_numpy(np.log10(histone[:, :, 3]+1e-4)[idx])\n",
    "\n",
    "        methylation = self.methylation[idx]\n",
    "        coordinates = self.coords[idx]\n",
    "\n",
    "        return sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation, coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = data['dna'].shape[0]\n",
    "split_index = int(0.8 * size) ### 80% of the data will be for training\n",
    "\n",
    "# I'm applying log10 in both cases\n",
    "train_dataset = MethDataset(sequence = data['dna'][:split_index],\n",
    "                           histone = data['histone'][:split_index], \n",
    "                           methylation = data['methyl'][:split_index],\n",
    "                           coords = data['coords'][:split_index],\n",
    "                           apply_log10=True)\n",
    "\n",
    "test_dataset = MethDataset(sequence = data['dna'][split_index:],\n",
    "                           histone = data['histone'][split_index:], \n",
    "                           methylation = data['methyl'][split_index:],\n",
    "                           coords = data['coords'][split_index:],\n",
    "                           apply_log10=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model class\n",
    "## 1- My idea is to be able to control de architecture of the model, and training parameters since the model is created.\n",
    "## This should make it easier to debug and to try different architectures, and the architecture of the model can be \n",
    "## specified through the arguments.\n",
    "## 2- The `forward` method unsqueezes the input so the model understands the structure in batches.\n",
    "## 3- There is a method called `training_loop`. Please, complete it, after you specify the architecture, add the loss function, and backward\n",
    "## propagation step\n",
    "## 4- I think we can add an `eval_loop` method, in which we iterate over the `test_dataloader` and evaluate the accuracy of the model (R^2)\n",
    "## 5- Try some architectures, and some way to pass arguments to the model, such that we can try different numbers without having problems\n",
    "## with tensor shapes and things like that. The idea is to be able to test certain combinations of numbers, so we can use Optuna to make\n",
    "## a bayesian search for \"optimal\" parameters. Look at papers where people use CNNs for DNA and histone marks, try to have a similar architecture\n",
    "## and let's start with that\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, DNA_in_channels, DNA_out_channels, epochs=100, learning_rate=1e-3, optimizer=torch.optim.SGD):\n",
    "        super().__init__()\n",
    "        # Module parameters\n",
    "        self.DNA_in_channels = DNA_in_channels\n",
    "        self.DNA_out_channels = DNA_out_channels\n",
    "\n",
    "\n",
    "        # Training parameters\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate=learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Modules and architecture\n",
    "        self.dna_module = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.DNA_in_channels, out_channels=self.DNA_out_channels, kernel_size=(50, 4), stride=50)\n",
    "        )\n",
    "\n",
    "\n",
    "    # self.attn = nn.MultiheadAttention(embed_dim=attn_dim, num_heads=n_heads, batch_first=True)\n",
    "\n",
    "    # self.fc = nn.Sequential(\n",
    "    #     nn.Linear(attn_dim, attn_dim // 2),\n",
    "    #     nn.ReLU(),\n",
    "    #     nn.Linear(attn_dim // 2, output_dim)\n",
    "    # )\n",
    "\n",
    "    def forward(self, sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation):\n",
    "        sequence = sequence.to(torch.float32).unsqueeze(1)\n",
    "        return self.dna_module(sequence)\n",
    "\n",
    "\n",
    "    def training_loop(self, loss_fn, train_dataloader):\n",
    "        optimizer = self.optimizer(self.parameters(), lr=self.learning_rate)\n",
    "        loss_fn = loss_fn()\n",
    "\n",
    "        self.train()\n",
    "        for e in range(self.epochs):\n",
    "            for i, (sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation, coordinates) in enumerate(train_dataloader):\n",
    "        \n",
    "                out = self.forward(sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation)\n",
    "\n",
    "                break\n",
    "    \n",
    "    def eval_loop(args, kwargs):\n",
    "        pass\n",
    "\n",
    "model = Model(DNA_in_channels=1, DNA_out_channels=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.1600, 0.6100, 0.4200, 0.4000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "model.training_loop(loss_fn=nn.MSELoss, train_dataloader=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
