{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import wandb\n",
    "wandb_logger = WandbLogger(project=\"MethPrediction\")\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_API_KEY'] = '2a1829519497eaab2f05c336830a1d4b0a3a8238'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train_loss_epoch</td><td>‚ñà‚ñà‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train_loss_relative</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train_loss_step</td><td>‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>trainer/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>299</td></tr><tr><td>train_loss_epoch</td><td>0.00096</td></tr><tr><td>train_loss_relative</td><td>0.89027</td></tr><tr><td>train_loss_step</td><td>0.00096</td></tr><tr><td>trainer/global_step</td><td>299</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-yogurt-5</strong> at: <a href='https://wandb.ai/andygglez-meth/my-awesome-project/runs/kae9abt0' target=\"_blank\">https://wandb.ai/andygglez-meth/my-awesome-project/runs/kae9abt0</a><br> View project at: <a href='https://wandb.ai/andygglez-meth/my-awesome-project' target=\"_blank\">https://wandb.ai/andygglez-meth/my-awesome-project</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251128_151453-kae9abt0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andy-bio/MyStuff/PhD/Codes/Pytorch/Methylation/wandb/run-20251128_153247-mm6429m7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andygglez-meth/my-awesome-project/runs/mm6429m7' target=\"_blank\">helpful-violet-6</a></strong> to <a href='https://wandb.ai/andygglez-meth/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andygglez-meth/my-awesome-project' target=\"_blank\">https://wandb.ai/andygglez-meth/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andygglez-meth/my-awesome-project/runs/mm6429m7' target=\"_blank\">https://wandb.ai/andygglez-meth/my-awesome-project/runs/mm6429m7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"andygglez-meth\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"my-awesome-project\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config={\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"architecture\": \"CNN+ATT\",\n",
    "        \"dataset\": \"test.for.torch.npz\",\n",
    "        \"epochs\": 100,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dataset Class\n",
    "## Notice that the __init__ method contains an argument `apply_log10`, if you set it to True\n",
    "## you will apply a log10 to the raw counts. We can experiment with this\n",
    "class MethDataset(Dataset):\n",
    "    def __init__(self, sequence, histone, methylation, coords, apply_log10=False):\n",
    "        self.sequence = sequence\n",
    "        self.histone = histone\n",
    "        self.methylation = methylation\n",
    "        self.transform = apply_log10\n",
    "        self.coords = coords\n",
    "        self.histone_names = ['H3K4me3', 'H3K36me2', 'H3K27me3', 'H3K9me3']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.methylation.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sequence = torch.from_numpy(self.sequence[idx])\n",
    "        histone = self.histone.astype(np.float32)\n",
    "\n",
    "        H3K4me3 = torch.from_numpy(histone[:, :, 0][idx].astype(np.float32)) if not self.transform else torch.from_numpy(np.log10(histone[:, :, 0]+1e-4)[idx])\n",
    "        H3K36me2 = torch.from_numpy(histone[:, :, 1][idx].astype(np.float32)) if not self.transform else torch.from_numpy(np.log10(histone[:, :, 1]+1e-4)[idx])\n",
    "        H3K27me3 = torch.from_numpy(histone[:, :, 2][idx].astype(np.float32)) if not self.transform else torch.from_numpy(np.log10(histone[:, :, 2]+1e-4)[idx])\n",
    "        H3K9me3 = torch.from_numpy(histone[:, :, 3][idx].astype(np.float32)) if not self.transform else torch.from_numpy(np.log10(histone[:, :, 3]+1e-4)[idx])\n",
    "\n",
    "        methylation = self.methylation[idx]\n",
    "        coordinates = self.coords[idx]\n",
    "\n",
    "        return sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation, coordinates\n",
    "\n",
    "class MethDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, npz_path, train_split=0.8, batch_size=32, apply_log10=True):\n",
    "        super().__init__()\n",
    "        self.npz_path = npz_path\n",
    "        self.batch_size = batch_size\n",
    "        self.train_split = train_split\n",
    "        self.transform = apply_log10\n",
    "        self.histone_names = ['H3K4me3', 'H3K36me2', 'H3K27me3', 'H3K9me3']\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        self.data = np.load(self.npz_path, allow_pickle=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        split_index = int(self.train_split * self.data['dna'].shape[0]) ### 80% of the data will be for training\n",
    "\n",
    "        self.train_dataset = MethDataset(sequence = data['dna'][:split_index],\n",
    "                                histone = data['histone'][:split_index], \n",
    "                                methylation = data['methyl'][:split_index],\n",
    "                                coords = data['coords'][:split_index],\n",
    "                                apply_log10=True)\n",
    "\n",
    "        self.test_dataset = MethDataset(sequence = data['dna'][split_index:],\n",
    "                                histone = data['histone'][split_index:], \n",
    "                                methylation = data['methyl'][split_index:],\n",
    "                                coords = data['coords'][split_index:],\n",
    "                                apply_log10=True)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = MethDataModule(npz_path='chr19.npz', train_split=0.8, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, DNA_kernel_sizes, DNA_strides, DNA_conv_channels, loss_fn=nn.MSELoss, optimizer=torch.optim.Adam, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        # Module parameters\n",
    "        self.DNA_layer1_kernel_size, self.DNA_layer2_kernel_size, self.DNA_layer3_kernel_size, self.DNA_layer4_kernel_size = DNA_kernel_sizes\n",
    "        self.DNA_conv_channels = DNA_conv_channels\n",
    "        self.DNA_layer1_stride, self.DNA_layer2_stride, self.DNA_layer3_stride, self.DNA_layer4_stride = DNA_strides\n",
    "\n",
    "        self.loss_fn = loss_fn()\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.first_epoch_loss = None\n",
    "        self.first_test_loss = None\n",
    "\n",
    "        \n",
    "        ############## Modules and architecture\n",
    "        self.dna_module = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=4, out_channels=DNA_conv_channels, kernel_size=(self.DNA_layer1_kernel_size), \n",
    "                        stride=self.DNA_layer1_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=DNA_conv_channels, out_channels=1, kernel_size=(self.DNA_layer3_kernel_size), \n",
    "                        stride=self.DNA_layer3_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=(self.DNA_layer4_kernel_size), \n",
    "                        stride=self.DNA_layer4_stride, padding=0)\n",
    "        )\n",
    "\n",
    "        ### \n",
    "        self.H3K4me3_module = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=DNA_conv_channels, kernel_size=(self.DNA_layer1_kernel_size), \n",
    "                        stride=self.DNA_layer1_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=DNA_conv_channels, out_channels=1, kernel_size=(self.DNA_layer3_kernel_size), \n",
    "                        stride=self.DNA_layer3_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=(self.DNA_layer4_kernel_size), \n",
    "                        stride=self.DNA_layer4_stride, padding=0)\n",
    "        )\n",
    "        self.H3K36me2_module = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=DNA_conv_channels, kernel_size=(self.DNA_layer1_kernel_size), \n",
    "                        stride=self.DNA_layer1_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=DNA_conv_channels, out_channels=1, kernel_size=(self.DNA_layer3_kernel_size), \n",
    "                        stride=self.DNA_layer3_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=(self.DNA_layer4_kernel_size), \n",
    "                        stride=self.DNA_layer4_stride, padding=0)\n",
    "        )\n",
    "        self.H3K27me3_module = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=DNA_conv_channels, kernel_size=(self.DNA_layer1_kernel_size), \n",
    "                        stride=self.DNA_layer1_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=DNA_conv_channels, out_channels=1, kernel_size=(self.DNA_layer3_kernel_size), \n",
    "                        stride=self.DNA_layer3_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=(self.DNA_layer4_kernel_size), \n",
    "                        stride=self.DNA_layer4_stride, padding=0)\n",
    "        )\n",
    "        self.H3K9me3_module = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=DNA_conv_channels, kernel_size=(self.DNA_layer1_kernel_size), \n",
    "                        stride=self.DNA_layer1_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=DNA_conv_channels, out_channels=1, kernel_size=(self.DNA_layer3_kernel_size), \n",
    "                        stride=self.DNA_layer3_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=(self.DNA_layer4_kernel_size), \n",
    "                        stride=self.DNA_layer4_stride, padding=0)\n",
    "        )\n",
    "        \n",
    "        #### Cross-Attention\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=25, num_heads=5, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(125, 250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3):\n",
    "        sequence = sequence.to(torch.float32).permute(0, 2, 1) ### Changed to (B,C=4,L=500) to use Conv1D\n",
    "        dna_module_output = self.dna_module(sequence)\n",
    "\n",
    "        H3K4me3_module_output = self.H3K4me3_module(H3K4me3.unsqueeze(1))\n",
    "        H3K36me2_module_output = self.H3K36me2_module(H3K36me2.unsqueeze(1))\n",
    "        H3K27me3_module_output = self.H3K27me3_module(H3K27me3.unsqueeze(1))\n",
    "        H3K9me3_module_output = self.H3K9me3_module(H3K9me3.unsqueeze(1))\n",
    "        \n",
    "        stack = torch.cat([dna_module_output, H3K4me3_module_output, H3K36me2_module_output, H3K27me3_module_output, H3K9me3_module_output], dim=1)#.permute(1,0,2) # Not sure if this is ok\n",
    "\n",
    "        ### Attention\n",
    "        attention_output, attention_weights = self.attn(stack, stack, stack)\n",
    "        attention_reshaped = attention_output.reshape(attention_output.size(0), -1)\n",
    "        ###\n",
    "\n",
    "        methylation_prediction = self.fc(attention_reshaped)\n",
    "\n",
    "        return methylation_prediction\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation, coordinates = batch\n",
    "        prediction = self.forward(sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3)\n",
    "        loss = self.loss_fn(prediction, methylation.unsqueeze(-1).float())\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        epoch_loss = self.trainer.callback_metrics[\"train_loss\"].item()\n",
    "\n",
    "        if self.current_epoch == 0:\n",
    "            self.first_epoch_loss = epoch_loss\n",
    "\n",
    "        if self.first_epoch_loss is not None:\n",
    "\n",
    "            relative = epoch_loss / self.first_epoch_loss * 100\n",
    "            print(\"train_loss_relative\", relative)\n",
    "            self.log(\"train_loss_relative\", relative)\n",
    "    \n",
    "\n",
    "    ############################# NOT USING THIS ######################################\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation, coordinates = batch\n",
    "        prediction = self.forward(sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3)\n",
    "        loss = loss_fn(prediction, methylation.unsqueeze(-1).float())\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "    ############################# NOT USING THIS ######################################\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation, coordinates = batch\n",
    "        prediction = self.forward(sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3)\n",
    "        loss = loss_fn(prediction, methylation.unsqueeze(-1).float())\n",
    "        self.log('test_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        epoch_loss = self.trainer.callback_metrics[\"test_loss\"].item()\n",
    "        if not hasattr(self, \"first_test_loss\") or self.first_test_loss is None:\n",
    "            self.first_test_loss = epoch_loss\n",
    "        relative = epoch_loss / self.first_test_loss * 100\n",
    "        print(\"test_loss_relative:\", relative)\n",
    "        self.log(\"test_loss_relative\", relative)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(DNA_kernel_sizes=(10,0,10,5), DNA_strides=(2,5,3,3), DNA_conv_channels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy-bio/.anaconda/envs/lightning/lib/python3.13/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/andy-bio/.anaconda/envs/lightning/lib/python3.13/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | loss_fn         | MSELoss            | 0      | train\n",
      "1 | dna_module      | Sequential         | 103    | train\n",
      "2 | H3K4me3_module  | Sequential         | 43     | train\n",
      "3 | H3K36me2_module | Sequential         | 43     | train\n",
      "4 | H3K27me3_module | Sequential         | 43     | train\n",
      "5 | H3K9me3_module  | Sequential         | 43     | train\n",
      "6 | attn            | MultiheadAttention | 2.6 K  | train\n",
      "7 | fc              | Sequential         | 57.6 K | train\n",
      "---------------------------------------------------------------\n",
      "60.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 K    Total params\n",
      "0.242     Total estimated model params size (MB)\n",
      "42        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/andy-bio/.anaconda/envs/lightning/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/andy-bio/.anaconda/envs/lightning/lib/python3.13/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.17it/s, v_num=29m7]train_loss_relative 100.0\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.50it/s, v_num=29m7]train_loss_relative 99.63172879191204\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.55it/s, v_num=29m7]train_loss_relative 99.3053757566455\n",
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.00it/s, v_num=29m7]train_loss_relative 98.92740664784868\n",
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.94it/s, v_num=29m7]train_loss_relative 98.41140500792122\n",
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.53it/s, v_num=29m7]train_loss_relative 97.74906329139141\n",
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.09it/s, v_num=29m7]train_loss_relative 96.9017835767504\n",
      "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.67it/s, v_num=29m7]train_loss_relative 95.79795219349982\n",
      "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.41it/s, v_num=29m7]train_loss_relative 94.45749064951518\n",
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.83it/s, v_num=29m7]train_loss_relative 92.70973802732941\n",
      "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.96it/s, v_num=29m7]train_loss_relative 90.60368186818367\n",
      "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.05it/s, v_num=29m7]train_loss_relative 88.34355221375336\n",
      "Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.62it/s, v_num=29m7]train_loss_relative 85.7620706525961\n",
      "Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.05it/s, v_num=29m7]train_loss_relative 83.0885766269387\n",
      "Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.99it/s, v_num=29m7]train_loss_relative 80.01001194114549\n",
      "Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.20it/s, v_num=29m7]train_loss_relative 76.7916646750923\n",
      "Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.05it/s, v_num=29m7]train_loss_relative 73.30784948221718\n",
      "Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.18it/s, v_num=29m7]train_loss_relative 70.34783160266387\n",
      "Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.40it/s, v_num=29m7]train_loss_relative 68.1776392556633\n",
      "Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.19it/s, v_num=29m7]train_loss_relative 67.03202804339597\n",
      "Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.86it/s, v_num=29m7]train_loss_relative 65.87263087099116\n",
      "Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.38it/s, v_num=29m7]train_loss_relative 63.88963844330847\n",
      "Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.11it/s, v_num=29m7]train_loss_relative 60.77244125005525\n",
      "Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.30it/s, v_num=29m7]train_loss_relative 57.233440713488804\n",
      "Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.94it/s, v_num=29m7]train_loss_relative 54.05685598347677\n",
      "Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.46it/s, v_num=29m7]train_loss_relative 51.25377531596514\n",
      "Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.17it/s, v_num=29m7]train_loss_relative 48.737426018056496\n",
      "Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.86it/s, v_num=29m7]train_loss_relative 46.0974941994172\n",
      "Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.72it/s, v_num=29m7]train_loss_relative 43.06804848147961\n",
      "Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.19it/s, v_num=29m7]train_loss_relative 39.88529361777834\n",
      "Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.15it/s, v_num=29m7]train_loss_relative 36.518579538384074\n",
      "Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.77it/s, v_num=29m7]train_loss_relative 34.36556257473592\n",
      "Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.53it/s, v_num=29m7]train_loss_relative 33.167097111886314\n",
      "Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.33it/s, v_num=29m7]train_loss_relative 35.200854798700426\n",
      "Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.92it/s, v_num=29m7]train_loss_relative 36.41813847758315\n",
      "Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.48it/s, v_num=29m7]train_loss_relative 31.382344439135885\n",
      "Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.49it/s, v_num=29m7]train_loss_relative 29.96118349527974\n",
      "Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.31it/s, v_num=29m7]train_loss_relative 32.6966002079169\n",
      "Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.20it/s, v_num=29m7]train_loss_relative 27.539027306092656\n",
      "Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.90it/s, v_num=29m7]train_loss_relative 30.43859067229348\n",
      "Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.29it/s, v_num=29m7]train_loss_relative 27.170641964365817\n",
      "Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.58it/s, v_num=29m7]train_loss_relative 28.777992917765605\n",
      "Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.69it/s, v_num=29m7]train_loss_relative 26.927910841844877\n",
      "Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.06it/s, v_num=29m7]train_loss_relative 27.077408615789306\n",
      "Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.87it/s, v_num=29m7]train_loss_relative 25.99673598544402\n",
      "Epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.03it/s, v_num=29m7]train_loss_relative 25.19470680001995\n",
      "Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.62it/s, v_num=29m7]train_loss_relative 25.23721293401132\n",
      "Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.80it/s, v_num=29m7]train_loss_relative 23.515538119009843\n",
      "Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.26it/s, v_num=29m7]train_loss_relative 24.48419204139911\n",
      "Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.14it/s, v_num=29m7]train_loss_relative 22.579490102088737\n",
      "Epoch 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.43it/s, v_num=29m7]train_loss_relative 23.28701836382581\n",
      "Epoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.57it/s, v_num=29m7]train_loss_relative 22.14426913901802\n",
      "Epoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.18it/s, v_num=29m7]train_loss_relative 22.141877520494425\n",
      "Epoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.75it/s, v_num=29m7]train_loss_relative 21.81265558057811\n",
      "Epoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.12it/s, v_num=29m7]train_loss_relative 21.065408812514193\n",
      "Epoch 55: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.12it/s, v_num=29m7]train_loss_relative 21.309992012720436\n",
      "Epoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.26it/s, v_num=29m7]train_loss_relative 20.238408570343363\n",
      "Epoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.60it/s, v_num=29m7]train_loss_relative 20.49338830747215\n",
      "Epoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.99it/s, v_num=29m7]train_loss_relative 19.812667616489115\n",
      "Epoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.57it/s, v_num=29m7]train_loss_relative 19.484450398454367\n",
      "Epoch 60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.30it/s, v_num=29m7]train_loss_relative 19.51756298810721\n",
      "Epoch 61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.38it/s, v_num=29m7]train_loss_relative 18.794242781064636\n",
      "Epoch 62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.71it/s, v_num=29m7]train_loss_relative 18.823552645386023\n",
      "Epoch 63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.87it/s, v_num=29m7]train_loss_relative 18.652559702787705\n",
      "Epoch 64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.53it/s, v_num=29m7]train_loss_relative 17.996962246736594\n",
      "Epoch 65: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.13it/s, v_num=29m7]train_loss_relative 17.983764247769052\n",
      "Epoch 66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.94it/s, v_num=29m7]train_loss_relative 18.080167398770552\n",
      "Epoch 67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.84it/s, v_num=29m7]train_loss_relative 17.665924544383703\n",
      "Epoch 68: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.26it/s, v_num=29m7]train_loss_relative 17.169564266127274\n",
      "Epoch 69: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.41it/s, v_num=29m7]train_loss_relative 17.07371103174942\n",
      "Epoch 70: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.81it/s, v_num=29m7]train_loss_relative 17.085823031850133\n",
      "Epoch 71: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.93it/s, v_num=29m7]train_loss_relative 16.827630259857393\n",
      "Epoch 72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.70it/s, v_num=29m7]train_loss_relative 16.44987212536273\n",
      "Epoch 73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.25it/s, v_num=29m7]train_loss_relative 16.31486067706703\n",
      "Epoch 74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.93it/s, v_num=29m7]train_loss_relative 16.326754785675355\n",
      "Epoch 75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.34it/s, v_num=29m7]train_loss_relative 16.255902006102758\n",
      "Epoch 76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.76it/s, v_num=29m7]train_loss_relative 16.10332608250396\n",
      "Epoch 77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.04it/s, v_num=29m7]train_loss_relative 15.89179321723844\n",
      "Epoch 78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.59it/s, v_num=29m7]train_loss_relative 15.686333644998868\n",
      "Epoch 79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.67it/s, v_num=29m7]train_loss_relative 15.512084435097629\n",
      "Epoch 80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.32it/s, v_num=29m7]train_loss_relative 15.385807668770624\n",
      "Epoch 81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.43it/s, v_num=29m7]train_loss_relative 15.303921970773903\n",
      "Epoch 82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.25it/s, v_num=29m7]train_loss_relative 15.289243693096166\n",
      "Epoch 83: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.97it/s, v_num=29m7]train_loss_relative 15.375584061603941\n",
      "Epoch 84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.38it/s, v_num=29m7]train_loss_relative 15.579423282031069\n",
      "Epoch 85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.83it/s, v_num=29m7]train_loss_relative 16.20374293313906\n",
      "Epoch 86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.17it/s, v_num=29m7]train_loss_relative 17.02550686230233\n",
      "Epoch 87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.81it/s, v_num=29m7]train_loss_relative 18.473606803526966\n",
      "Epoch 88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.65it/s, v_num=29m7]train_loss_relative 17.19169235768763\n",
      "Epoch 89: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.46it/s, v_num=29m7]train_loss_relative 15.419271305827865\n",
      "Epoch 90: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.92it/s, v_num=29m7]train_loss_relative 15.101326115291439\n",
      "Epoch 91: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.69it/s, v_num=29m7]train_loss_relative 16.339816170135766\n",
      "Epoch 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.12it/s, v_num=29m7]train_loss_relative 16.17390217446717\n",
      "Epoch 93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.67it/s, v_num=29m7]train_loss_relative 14.791787040187563\n",
      "Epoch 94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.67it/s, v_num=29m7]train_loss_relative 15.711135230584883\n",
      "Epoch 95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.36it/s, v_num=29m7]train_loss_relative 16.419538516886625\n",
      "Epoch 96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.72it/s, v_num=29m7]train_loss_relative 14.843065900693912\n",
      "Epoch 97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.72it/s, v_num=29m7]train_loss_relative 15.130058394228241\n",
      "Epoch 98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.70it/s, v_num=29m7]train_loss_relative 16.168022562767796\n",
      "Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.76it/s, v_num=29m7]train_loss_relative 15.014112451516548\n",
      "Epoch 100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.37it/s, v_num=29m7]train_loss_relative 14.63710829075745\n",
      "Epoch 101: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.66it/s, v_num=29m7]train_loss_relative 15.445918051908947\n",
      "Epoch 102: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.97it/s, v_num=29m7]train_loss_relative 15.02756292792765\n",
      "Epoch 103: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.45it/s, v_num=29m7]train_loss_relative 14.466949734231175\n",
      "Epoch 104: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.11it/s, v_num=29m7]train_loss_relative 14.791475766627006\n",
      "Epoch 105: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.48it/s, v_num=29m7]train_loss_relative 14.913076512355449\n",
      "Epoch 106: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.42it/s, v_num=29m7]train_loss_relative 14.530984758178834\n",
      "Epoch 107: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.22it/s, v_num=29m7]train_loss_relative 14.411856907959244\n",
      "Epoch 108: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.50it/s, v_num=29m7]train_loss_relative 14.694325559080172\n",
      "Epoch 109: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.41it/s, v_num=29m7]train_loss_relative 14.696162073087446\n",
      "Epoch 110: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.32it/s, v_num=29m7]train_loss_relative 14.341444234615322\n",
      "Epoch 111: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.63it/s, v_num=29m7]train_loss_relative 14.36645592985471\n",
      "Epoch 112: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.22it/s, v_num=29m7]train_loss_relative 14.58653152505991\n",
      "Epoch 113: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.57it/s, v_num=29m7]train_loss_relative 14.436076584916776\n",
      "Epoch 114: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.35it/s, v_num=29m7]train_loss_relative 14.21883530841019\n",
      "Epoch 115: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.90it/s, v_num=29m7]train_loss_relative 14.257408155104434\n",
      "Epoch 116: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.64it/s, v_num=29m7]train_loss_relative 14.360054070292625\n",
      "Epoch 117: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.99it/s, v_num=29m7]train_loss_relative 14.287227297556843\n",
      "Epoch 118: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.66it/s, v_num=29m7]train_loss_relative 14.127880309367184\n",
      "Epoch 119: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.44it/s, v_num=29m7]train_loss_relative 14.135927595556314\n",
      "Epoch 120: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.70it/s, v_num=29m7]train_loss_relative 14.220274083978978\n",
      "Epoch 121: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.94it/s, v_num=29m7]train_loss_relative 14.165216708307\n",
      "Epoch 122: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.95it/s, v_num=29m7]train_loss_relative 14.048605850684009\n",
      "Epoch 123: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.72it/s, v_num=29m7]train_loss_relative 14.004827818338258\n",
      "Epoch 124: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.96it/s, v_num=29m7]train_loss_relative 14.043416228710532\n",
      "Epoch 125: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.44it/s, v_num=29m7]train_loss_relative 14.060554432165432\n",
      "Epoch 126: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.77it/s, v_num=29m7]train_loss_relative 13.992654428174214\n",
      "Epoch 127: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.18it/s, v_num=29m7]train_loss_relative 13.912056192209901\n",
      "Epoch 128: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.83it/s, v_num=29m7]train_loss_relative 13.879718327863335\n",
      "Epoch 129: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.00it/s, v_num=29m7]train_loss_relative 13.891507813969362\n",
      "Epoch 130: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.46it/s, v_num=29m7]train_loss_relative 13.899167737505358\n",
      "Epoch 131: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.20it/s, v_num=29m7]train_loss_relative 13.865922856589286\n",
      "Epoch 132: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.85it/s, v_num=29m7]train_loss_relative 13.808356270159718\n",
      "Epoch 133: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.97it/s, v_num=29m7]train_loss_relative 13.751805646045852\n",
      "Epoch 134: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.06it/s, v_num=29m7]train_loss_relative 13.717292325370517\n",
      "Epoch 135: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.19it/s, v_num=29m7]train_loss_relative 13.70316223501887\n",
      "Epoch 136: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.10it/s, v_num=29m7]train_loss_relative 13.698226819786512\n",
      "Epoch 137: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.59it/s, v_num=29m7]train_loss_relative 13.69580147996052\n",
      "Epoch 138: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.93it/s, v_num=29m7]train_loss_relative 13.688487415936251\n",
      "Epoch 139: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.95it/s, v_num=29m7]train_loss_relative 13.686850635796999\n",
      "Epoch 140: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.00it/s, v_num=29m7]train_loss_relative 13.689441988188623\n",
      "Epoch 141: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.87it/s, v_num=29m7]train_loss_relative 13.718036787969512\n",
      "Epoch 142: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.30it/s, v_num=29m7]train_loss_relative 13.762137334309948\n",
      "Epoch 143: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.36it/s, v_num=29m7]train_loss_relative 13.886006918435328\n",
      "Epoch 144: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.33it/s, v_num=29m7]train_loss_relative 14.03527988369713\n",
      "Epoch 145: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.28it/s, v_num=29m7]train_loss_relative 14.402078594913913\n",
      "Epoch 146: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.55it/s, v_num=29m7]train_loss_relative 14.658477220689564\n",
      "Epoch 147: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.67it/s, v_num=29m7]train_loss_relative 15.130571995603157\n",
      "Epoch 148: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.04it/s, v_num=29m7]train_loss_relative 14.634635395248596\n",
      "Epoch 149: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.19it/s, v_num=29m7]train_loss_relative 14.027831799112068\n",
      "Epoch 150: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.09it/s, v_num=29m7]train_loss_relative 13.31390427175925\n",
      "Epoch 151: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.67it/s, v_num=29m7]train_loss_relative 13.37628349329451\n",
      "Epoch 152: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.17it/s, v_num=29m7]train_loss_relative 13.909656791847288\n",
      "Epoch 153: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.02it/s, v_num=29m7]train_loss_relative 13.954922020093468\n",
      "Epoch 154: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.06it/s, v_num=29m7]train_loss_relative 13.660292948540187\n",
      "Epoch 155: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.24it/s, v_num=29m7]train_loss_relative 13.153792934048672\n",
      "Epoch 156: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.20it/s, v_num=29m7]train_loss_relative 13.044048252382263\n",
      "Epoch 157: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.96it/s, v_num=29m7]train_loss_relative 13.291198594814308\n",
      "Epoch 158: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.34it/s, v_num=29m7]train_loss_relative 13.472992729963892\n",
      "Epoch 159: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.55it/s, v_num=29m7]train_loss_relative 13.474437558074134\n",
      "Epoch 160: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.83it/s, v_num=29m7]train_loss_relative 13.119707614519099\n",
      "Epoch 161: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.97it/s, v_num=29m7]train_loss_relative 12.837404975963802\n",
      "Epoch 162: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.85it/s, v_num=29m7]train_loss_relative 12.775311088525717\n",
      "Epoch 163: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.23it/s, v_num=29m7]train_loss_relative 12.890866209989154\n",
      "Epoch 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.22it/s, v_num=29m7]train_loss_relative 13.02363216540496\n",
      "Epoch 165: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.69it/s, v_num=29m7]train_loss_relative 12.966228997594687\n",
      "Epoch 166: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.80it/s, v_num=29m7]train_loss_relative 12.835889246653654\n",
      "Epoch 167: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.99it/s, v_num=29m7]train_loss_relative 12.62243253785415\n",
      "Epoch 168: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.09it/s, v_num=29m7]train_loss_relative 12.471999213930502\n",
      "Epoch 169: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.59it/s, v_num=29m7]train_loss_relative 12.404817733074898\n",
      "Epoch 170: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.07it/s, v_num=29m7]train_loss_relative 12.406536654848185\n",
      "Epoch 171: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.66it/s, v_num=29m7]train_loss_relative 12.447365370207676\n",
      "Epoch 172: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.56it/s, v_num=29m7]train_loss_relative 12.481378923888565\n",
      "Epoch 173: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.61it/s, v_num=29m7]train_loss_relative 12.549049795953263\n",
      "Epoch 174: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.98it/s, v_num=29m7]train_loss_relative 12.55438208497533\n",
      "Epoch 175: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.22it/s, v_num=29m7]train_loss_relative 12.633596018243951\n",
      "Epoch 176: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.24it/s, v_num=29m7]train_loss_relative 12.58507452269485\n",
      "Epoch 177: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.63it/s, v_num=29m7]train_loss_relative 12.634038718418964\n",
      "Epoch 178: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.54it/s, v_num=29m7]train_loss_relative 12.485787768014317\n",
      "Epoch 179: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.33it/s, v_num=29m7]train_loss_relative 12.413951018130854\n",
      "Epoch 180: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.87it/s, v_num=29m7]train_loss_relative 12.162522393538735\n",
      "Epoch 181: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.66it/s, v_num=29m7]train_loss_relative 11.975931186961484\n",
      "Epoch 182: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.50it/s, v_num=29m7]train_loss_relative 11.748429123390707\n",
      "Epoch 183: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.38it/s, v_num=29m7]train_loss_relative 11.580566209373526\n",
      "Epoch 184: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.10it/s, v_num=29m7]train_loss_relative 11.4467350066616\n",
      "Epoch 185: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.99it/s, v_num=29m7]train_loss_relative 11.345680909876137\n",
      "Epoch 186: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.05it/s, v_num=29m7]train_loss_relative 11.264492118795548\n",
      "Epoch 187: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.83it/s, v_num=29m7]train_loss_relative 11.20195726048001\n",
      "Epoch 188: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.53it/s, v_num=29m7]train_loss_relative 11.164979690783612\n",
      "Epoch 189: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.18it/s, v_num=29m7]train_loss_relative 11.18997841629131\n",
      "Epoch 190: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.66it/s, v_num=29m7]train_loss_relative 11.414088462700816\n",
      "Epoch 191: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.47it/s, v_num=29m7]train_loss_relative 11.96441752381606\n",
      "Epoch 192: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.72it/s, v_num=29m7]train_loss_relative 13.855394028403506\n",
      "Epoch 193: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.64it/s, v_num=29m7]train_loss_relative 15.792232368894862\n",
      "Epoch 194: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.93it/s, v_num=29m7]train_loss_relative 20.69361329600172\n",
      "Epoch 195: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.38it/s, v_num=29m7]train_loss_relative 13.28975981924552\n",
      "Epoch 196: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.69it/s, v_num=29m7]train_loss_relative 10.859369579732965\n",
      "Epoch 197: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.13it/s, v_num=29m7]train_loss_relative 14.008736030820787\n",
      "Epoch 198: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.25it/s, v_num=29m7]train_loss_relative 12.12035433722008\n",
      "Epoch 199: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.78it/s, v_num=29m7]train_loss_relative 10.3202048636563\n",
      "Epoch 200: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.92it/s, v_num=29m7]train_loss_relative 11.615070883561069\n",
      "Epoch 201: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.85it/s, v_num=29m7]train_loss_relative 11.3173723088412\n",
      "Epoch 202: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.20it/s, v_num=29m7]train_loss_relative 10.045101290837621\n",
      "Epoch 203: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.69it/s, v_num=29m7]train_loss_relative 10.502987292360649\n",
      "Epoch 204: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.48it/s, v_num=29m7]train_loss_relative 10.600234340570767\n",
      "Epoch 205: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.25it/s, v_num=29m7]train_loss_relative 9.525757783378088\n",
      "Epoch 206: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.10it/s, v_num=29m7]train_loss_relative 9.779186340596775\n",
      "Epoch 207: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.45it/s, v_num=29m7]train_loss_relative 9.290683125798028\n",
      "Epoch 208: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.81it/s, v_num=29m7]train_loss_relative 8.381721096537898\n",
      "Epoch 209: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.22it/s, v_num=29m7]train_loss_relative 8.049888781956813\n",
      "Epoch 210: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.87it/s, v_num=29m7]train_loss_relative 7.525496590274831\n",
      "Epoch 211: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.33it/s, v_num=29m7]train_loss_relative 6.412271230361319\n",
      "Epoch 212: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.71it/s, v_num=29m7]train_loss_relative 5.440233504974186\n",
      "Epoch 213: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.70it/s, v_num=29m7]train_loss_relative 4.7663854748135766\n",
      "Epoch 214: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.93it/s, v_num=29m7]train_loss_relative 4.240886965018455\n",
      "Epoch 215: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.94it/s, v_num=29m7]train_loss_relative 3.8187044732154978\n",
      "Epoch 216: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.04it/s, v_num=29m7]train_loss_relative 4.134576668303504\n",
      "Epoch 217: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.69it/s, v_num=29m7]train_loss_relative 5.0843851382231\n",
      "Epoch 218: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.69it/s, v_num=29m7]train_loss_relative 8.816501088731158\n",
      "Epoch 219: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.57it/s, v_num=29m7]train_loss_relative 9.57245832859794\n",
      "Epoch 220: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.86it/s, v_num=29m7]train_loss_relative 13.26113475675735\n",
      "Epoch 221: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.65it/s, v_num=29m7]train_loss_relative 3.885992738195348\n",
      "Epoch 222: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.77it/s, v_num=29m7]train_loss_relative 3.819300648548839\n",
      "Epoch 223: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.45it/s, v_num=29m7]train_loss_relative 9.451988543472845\n",
      "Epoch 224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.75it/s, v_num=29m7]train_loss_relative 3.3678530379988234\n",
      "Epoch 225: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.67it/s, v_num=29m7]train_loss_relative 2.873601854277352\n",
      "Epoch 226: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.98it/s, v_num=29m7]train_loss_relative 6.298072742832632\n",
      "Epoch 227: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.05it/s, v_num=29m7]train_loss_relative 1.9447643596891884\n",
      "Epoch 228: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.11it/s, v_num=29m7]train_loss_relative 3.6697471047581485\n",
      "Epoch 229: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.75it/s, v_num=29m7]train_loss_relative 5.025659922429245\n",
      "Epoch 230: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.36it/s, v_num=29m7]train_loss_relative 1.6514724674731234\n",
      "Epoch 231: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.75it/s, v_num=29m7]train_loss_relative 5.504753596887043\n",
      "Epoch 232: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.85it/s, v_num=29m7]train_loss_relative 3.310352381038681\n",
      "Epoch 233: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.07it/s, v_num=29m7]train_loss_relative 2.825081871863616\n",
      "Epoch 234: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.93it/s, v_num=29m7]train_loss_relative 4.344250971017872\n",
      "Epoch 235: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.30it/s, v_num=29m7]train_loss_relative 1.38680628623181\n",
      "Epoch 236: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.12it/s, v_num=29m7]train_loss_relative 3.330344141627533\n",
      "Epoch 237: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.41it/s, v_num=29m7]train_loss_relative 1.552297144404024\n",
      "Epoch 238: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.53it/s, v_num=29m7]train_loss_relative 2.3834177622516064\n",
      "Epoch 239: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.41it/s, v_num=29m7]train_loss_relative 2.2600931230193924\n",
      "Epoch 240: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.35it/s, v_num=29m7]train_loss_relative 1.5089114464418907\n",
      "Epoch 241: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.94it/s, v_num=29m7]train_loss_relative 2.4843877719432816\n",
      "Epoch 242: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.19it/s, v_num=29m7]train_loss_relative 1.054313271902228\n",
      "Epoch 243: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.17it/s, v_num=29m7]train_loss_relative 1.9535243326346743\n",
      "Epoch 244: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.86it/s, v_num=29m7]train_loss_relative 1.132945945028951\n",
      "Epoch 245: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.80it/s, v_num=29m7]train_loss_relative 1.460148930045556\n",
      "Epoch 246: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.05it/s, v_num=29m7]train_loss_relative 1.4494974298142105\n",
      "Epoch 247: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.13it/s, v_num=29m7]train_loss_relative 1.0268204669905803\n",
      "Epoch 248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.89it/s, v_num=29m7]train_loss_relative 1.5785394510185804\n",
      "Epoch 249: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.92it/s, v_num=29m7]train_loss_relative 0.8692609239899708\n",
      "Epoch 250: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.02it/s, v_num=29m7]train_loss_relative 1.278701627137576\n",
      "Epoch 251: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.23it/s, v_num=29m7]train_loss_relative 0.9781510084191197\n",
      "Epoch 252: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.65it/s, v_num=29m7]train_loss_relative 0.9422331738828756\n",
      "Epoch 253: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.73it/s, v_num=29m7]train_loss_relative 1.0927327515192398\n",
      "Epoch 254: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.40it/s, v_num=29m7]train_loss_relative 0.7474421246474914\n",
      "Epoch 255: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.98it/s, v_num=29m7]train_loss_relative 1.0875613952512244\n",
      "Epoch 256: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.02it/s, v_num=29m7]train_loss_relative 0.8230157784740776\n",
      "Epoch 257: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.98it/s, v_num=29m7]train_loss_relative 0.8206631231860965\n",
      "Epoch 258: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.76it/s, v_num=29m7]train_loss_relative 0.9242250256748809\n",
      "Epoch 259: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.14it/s, v_num=29m7]train_loss_relative 0.6670297881707332\n",
      "Epoch 260: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.37it/s, v_num=29m7]train_loss_relative 0.8549365038251716\n",
      "Epoch 261: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.87it/s, v_num=29m7]train_loss_relative 0.6811388027083843\n",
      "Epoch 262: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.68it/s, v_num=29m7]train_loss_relative 0.7213746932485525\n",
      "Epoch 263: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.00it/s, v_num=29m7]train_loss_relative 0.7421624170793174\n",
      "Epoch 264: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.60it/s, v_num=29m7]train_loss_relative 0.615322386113132\n",
      "Epoch 265: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.40it/s, v_num=29m7]train_loss_relative 0.7325681121023369\n",
      "Epoch 266: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.02it/s, v_num=29m7]train_loss_relative 0.6054451788636674\n",
      "Epoch 267: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.76it/s, v_num=29m7]train_loss_relative 0.6357992687077313\n",
      "Epoch 268: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.34it/s, v_num=29m7]train_loss_relative 0.6378880980369221\n",
      "Epoch 269: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.75it/s, v_num=29m7]train_loss_relative 0.5567350277590303\n",
      "Epoch 270: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.94it/s, v_num=29m7]train_loss_relative 0.6213356400894614\n",
      "Epoch 271: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.15it/s, v_num=29m7]train_loss_relative 0.5492011808231692\n",
      "Epoch 272: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.08it/s, v_num=29m7]train_loss_relative 0.5601217489465292\n",
      "Epoch 273: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.67it/s, v_num=29m7]train_loss_relative 0.5690043939375807\n",
      "Epoch 274: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.25it/s, v_num=29m7]train_loss_relative 0.5129660742254633\n",
      "Epoch 275: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.77it/s, v_num=29m7]train_loss_relative 0.5522288566050555\n",
      "Epoch 276: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.45it/s, v_num=29m7]train_loss_relative 0.5170746690625969\n",
      "Epoch 277: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.07it/s, v_num=29m7]train_loss_relative 0.5017465537866532\n",
      "Epoch 278: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.55it/s, v_num=29m7]train_loss_relative 0.5228331758923193\n",
      "Epoch 279: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.86it/s, v_num=29m7]train_loss_relative 0.4812863697216045\n",
      "Epoch 280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.31it/s, v_num=29m7]train_loss_relative 0.48971669531997436\n",
      "Epoch 281: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.70it/s, v_num=29m7]train_loss_relative 0.48880967875046766\n",
      "Epoch 282: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.83it/s, v_num=29m7]train_loss_relative 0.46054766066851743\n",
      "Epoch 283: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.50it/s, v_num=29m7]train_loss_relative 0.4745574567587416\n",
      "Epoch 284: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.64it/s, v_num=29m7]train_loss_relative 0.46257920701607835\n",
      "Epoch 285: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.30it/s, v_num=29m7]train_loss_relative 0.44673359944571167\n",
      "Epoch 286: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.81it/s, v_num=29m7]train_loss_relative 0.4569198645932343\n",
      "Epoch 287: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.39it/s, v_num=29m7]train_loss_relative 0.4418001026528324\n",
      "Epoch 288: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.03it/s, v_num=29m7]train_loss_relative 0.43420854041095164\n",
      "Epoch 289: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.93it/s, v_num=29m7]train_loss_relative 0.43950856722543963\n",
      "Epoch 290: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.68it/s, v_num=29m7]train_loss_relative 0.4253629402160391\n",
      "Epoch 291: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.23it/s, v_num=29m7]train_loss_relative 0.42188580919055246\n",
      "Epoch 292: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.44it/s, v_num=29m7]train_loss_relative 0.42361856534430975\n",
      "Epoch 293: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.46it/s, v_num=29m7]train_loss_relative 0.41176287956611507\n",
      "Epoch 294: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.64it/s, v_num=29m7]train_loss_relative 0.4098097460547287\n",
      "Epoch 295: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.11it/s, v_num=29m7]train_loss_relative 0.409738736773727\n",
      "Epoch 296: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.10it/s, v_num=29m7]train_loss_relative 0.4001528351020704\n",
      "Epoch 297: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.02it/s, v_num=29m7]train_loss_relative 0.3981770856210499\n",
      "Epoch 298: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.48it/s, v_num=29m7]train_loss_relative 0.39762833086919963\n",
      "Epoch 299: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.58it/s, v_num=29m7]train_loss_relative 0.3899053149925879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.61it/s, v_num=29m7]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=300, logger=wandb_logger)\n",
    "trainer.fit(model=model, train_dataloaders=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy-bio/.anaconda/envs/lightning/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/andy-bio/.anaconda/envs/lightning/lib/python3.13/site-packages/pytorch_lightning/utilities/data.py:106: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "trainer.test(model, dataloaders=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model class\n",
    "## 1- My idea is to be able to control de architecture of the model, and training parameters since the model is created.\n",
    "## This should make it easier to debug and to try different architectures, and the architecture of the model can be \n",
    "## specified through the arguments.\n",
    "## 2- The `forward` method unsqueezes the input so the model understands the structure in batches.\n",
    "## 3- There is a method called `training_loop`. Please, complete it, after you specify the architecture, add the loss function, and backward\n",
    "## propagation step\n",
    "## 4- I think we can add an `eval_loop` method, in which we iterate over the `test_dataloader` and evaluate the accuracy of the model (R^2)\n",
    "## 5- Try some architectures, and some way to pass arguments to the model, such that we can try different numbers without having problems\n",
    "## with tensor shapes and things like that. The idea is to be able to test certain combinations of numbers, so we can use Optuna to make\n",
    "## a bayesian search for \"optimal\" parameters. Look at papers where people use CNNs for DNA and histone marks, try to have a similar architecture\n",
    "## and let's start with that\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, DNA_kernel_sizes, DNA_strides, DNA_conv_channels):\n",
    "        super().__init__()\n",
    "        # Module parameters\n",
    "        self.DNA_layer1_kernel_size, self.DNA_layer2_kernel_size, self.DNA_layer3_kernel_size, self.DNA_layer4_kernel_size = DNA_kernel_sizes\n",
    "        self.DNA_conv_channels = DNA_conv_channels\n",
    "        self.DNA_layer1_stride, self.DNA_layer2_stride, self.DNA_layer3_stride, self.DNA_layer4_stride = DNA_strides\n",
    "\n",
    "        \n",
    "        ############## Modules and architecture\n",
    "        self.dna_module = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=4, out_channels=DNA_conv_channels, kernel_size=(self.DNA_layer1_kernel_size), \n",
    "                        stride=self.DNA_layer1_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=DNA_conv_channels, out_channels=1, kernel_size=(self.DNA_layer3_kernel_size), \n",
    "                        stride=self.DNA_layer3_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=(self.DNA_layer4_kernel_size), \n",
    "                        stride=self.DNA_layer4_stride, padding=0)\n",
    "        )\n",
    "\n",
    "        ### \n",
    "        self.H3K4me3_module = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=DNA_conv_channels, kernel_size=(self.DNA_layer1_kernel_size), \n",
    "                        stride=self.DNA_layer1_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=DNA_conv_channels, out_channels=1, kernel_size=(self.DNA_layer3_kernel_size), \n",
    "                        stride=self.DNA_layer3_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=(self.DNA_layer4_kernel_size), \n",
    "                        stride=self.DNA_layer4_stride, padding=0)\n",
    "        )\n",
    "        self.H3K36me2_module = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=DNA_conv_channels, kernel_size=(self.DNA_layer1_kernel_size), \n",
    "                        stride=self.DNA_layer1_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=DNA_conv_channels, out_channels=1, kernel_size=(self.DNA_layer3_kernel_size), \n",
    "                        stride=self.DNA_layer3_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=(self.DNA_layer4_kernel_size), \n",
    "                        stride=self.DNA_layer4_stride, padding=0)\n",
    "        )\n",
    "        self.H3K27me3_module = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=DNA_conv_channels, kernel_size=(self.DNA_layer1_kernel_size), \n",
    "                        stride=self.DNA_layer1_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=DNA_conv_channels, out_channels=1, kernel_size=(self.DNA_layer3_kernel_size), \n",
    "                        stride=self.DNA_layer3_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=(self.DNA_layer4_kernel_size), \n",
    "                        stride=self.DNA_layer4_stride, padding=0)\n",
    "        )\n",
    "        self.H3K9me3_module = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=DNA_conv_channels, kernel_size=(self.DNA_layer1_kernel_size), \n",
    "                        stride=self.DNA_layer1_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=DNA_conv_channels, out_channels=1, kernel_size=(self.DNA_layer3_kernel_size), \n",
    "                        stride=self.DNA_layer3_stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=(self.DNA_layer4_kernel_size), \n",
    "                        stride=self.DNA_layer4_stride, padding=0)\n",
    "        )\n",
    "        \n",
    "        #### Cross-Attention\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=25, num_heads=5, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(125, 250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3):\n",
    "        sequence = sequence.to(torch.float32).permute(0, 2, 1) ### Changed to (B,C=4,L=500) to use Conv1D\n",
    "        dna_module_output = self.dna_module(sequence)\n",
    "\n",
    "        H3K4me3_module_output = self.H3K4me3_module(H3K4me3.unsqueeze(1))\n",
    "        H3K36me2_module_output = self.H3K36me2_module(H3K36me2.unsqueeze(1))\n",
    "        H3K27me3_module_output = self.H3K27me3_module(H3K27me3.unsqueeze(1))\n",
    "        H3K9me3_module_output = self.H3K9me3_module(H3K9me3.unsqueeze(1))\n",
    "        \n",
    "        stack = torch.cat([dna_module_output, H3K4me3_module_output, H3K36me2_module_output, H3K27me3_module_output, H3K9me3_module_output], dim=1)#.permute(1,0,2) # Not sure if this is ok\n",
    "\n",
    "        ### Attention\n",
    "        attention_output, attention_weights = self.attn(stack, stack, stack)\n",
    "        attention_reshaped = attention_output.reshape(attention_output.size(0), -1)\n",
    "        ###\n",
    "\n",
    "        methylation_prediction = self.fc(attention_reshaped)\n",
    "\n",
    "        return methylation_prediction\n",
    "\n",
    "\n",
    "    def training_loop(self, loss_fn, train_dataset, batch_size=10, epochs=100, learning_rate=1e-3, optimizer=torch.optim.SGD):\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = optimizer(self.parameters(), lr=learning_rate)\n",
    "        loss_fn = loss_fn()\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Model will be trained in {device}\")\n",
    "        self.to(device)\n",
    "        \n",
    "        self.train()\n",
    "        loss_dict = {}\n",
    "        for e in range(epochs):\n",
    "            loss_accum = 0\n",
    "            for i, (sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation, coordinates) in enumerate(train_dataloader):\n",
    "                \n",
    "                sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation = sequence.to(device), H3K4me3.to(device), H3K36me2.to(device), H3K27me3.to(device), H3K9me3.to(device), methylation.to(device)\n",
    "                prediction = self.forward(sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3)\n",
    "\n",
    "                loss = loss_fn(prediction, methylation.unsqueeze(-1).float())\n",
    "                \n",
    "                loss_accum += loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(\"---\")\n",
    "\n",
    "            if (e+1) % 2 == 0:\n",
    "                # print(f\"Iter: {e+1}, Loss: {loss_accum}\")\n",
    "                loss_dict[e+1] = loss_accum\n",
    "        \n",
    "        with open(\"loss_dict.pkl\", \"wb\") as file:\n",
    "            pickle.dump(loss_dict, file)\n",
    "    \n",
    "    def eval_loop(args, kwargs):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(DNA_kernel_sizes=(10,0,10,5), DNA_strides=(2,5,3,3), DNA_conv_channels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pth\")\n",
    "\n",
    "with open(\"train_dataset.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train_dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[694], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mtraining_loop(loss_fn\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss, train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam)\n",
      "Cell \u001b[0;32mIn[692], line 125\u001b[0m, in \u001b[0;36mModel.training_loop\u001b[0;34m(self, loss_fn, train_dataset, batch_size, epochs, learning_rate, optimizer)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    124\u001b[0m     loss_accum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation, coordinates) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    127\u001b[0m         sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mto(device), H3K4me3\u001b[38;5;241m.\u001b[39mto(device), H3K36me2\u001b[38;5;241m.\u001b[39mto(device), H3K27me3\u001b[38;5;241m.\u001b[39mto(device), H3K9me3\u001b[38;5;241m.\u001b[39mto(device), methylation\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    128\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3)\n",
      "File \u001b[0;32m~/.anaconda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.anaconda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.anaconda/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[472], line 21\u001b[0m, in \u001b[0;36mMethDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m sequence \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence[idx])\n\u001b[1;32m     19\u001b[0m histone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistone\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 21\u001b[0m H3K4me3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(histone[:, :, \u001b[38;5;241m0\u001b[39m][idx]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mlog10(histone[:, :, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)[idx])\n\u001b[1;32m     22\u001b[0m H3K36me2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(histone[:, :, \u001b[38;5;241m1\u001b[39m][idx]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mlog10(histone[:, :, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)[idx])\n\u001b[1;32m     23\u001b[0m H3K27me3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(histone[:, :, \u001b[38;5;241m2\u001b[39m][idx]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mlog10(histone[:, :, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)[idx])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.training_loop(loss_fn=nn.MSELoss, train_dataset=train_dataset, batch_size=10, epochs=150, learning_rate=1e-3, optimizer=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3323e-01,  1.1601e+00,  1.4217e+00, -2.4813e-01,  4.2929e-01,\n",
       "         -5.4772e-01,  5.7561e-01,  1.2113e+00, -2.6437e-01,  1.3577e+00,\n",
       "          5.2429e-01, -5.2623e-01, -1.7426e+00, -8.3357e-01, -1.3284e+00,\n",
       "         -8.0573e-02, -6.7825e-01,  5.6580e-01, -1.4961e+00,  3.0421e-01,\n",
       "         -6.0418e-01,  1.2223e+00,  2.6899e-01,  5.1948e-01, -3.0307e-01,\n",
       "         -3.5551e-01,  2.7880e-01, -9.3031e-01,  8.4154e-01,  3.5884e-01,\n",
       "          2.7014e-01,  2.2733e+00,  1.1548e-01, -2.4733e+00,  2.1760e+00,\n",
       "         -1.2356e+00,  6.0599e-01,  8.1846e-01,  7.7446e-01, -6.3601e-01,\n",
       "         -1.7114e-01, -5.5536e-01, -6.5090e-01, -1.1438e-01,  1.3795e-01,\n",
       "         -1.0505e+00,  9.5566e-01, -6.4408e-01,  7.0710e-01,  1.8673e+00,\n",
       "          6.0603e-01, -1.0644e-01, -6.3872e-01, -9.0523e-01, -2.3540e-01,\n",
       "         -7.6514e-01, -1.2568e+00,  1.4438e+00,  1.0096e+00, -1.1652e+00,\n",
       "         -1.1298e+00, -1.1263e+00, -8.0377e-01, -2.7690e-01, -9.2203e-01,\n",
       "          1.9362e+00,  1.2261e+00,  7.4439e-01,  8.3771e-01,  7.2296e-01,\n",
       "          1.9316e-01, -3.4615e-01,  3.3194e-01, -1.5335e+00, -2.9190e-01,\n",
       "         -9.0494e-01, -6.2645e-02, -1.6627e+00, -5.0839e-01, -1.2216e+00,\n",
       "          6.9030e-01,  1.1151e+00, -1.2670e+00,  1.2556e+00, -3.0261e+00,\n",
       "         -1.0851e+00, -7.5479e-01,  4.0401e-01, -8.3331e-01,  1.2681e-01,\n",
       "          8.0894e-02,  8.0486e-01, -2.3931e+00,  2.2070e+00,  5.2127e-01,\n",
       "         -1.4470e-01, -1.6115e-01, -1.8292e-01,  1.2150e-01, -3.5840e-02,\n",
       "          9.7673e-01, -4.2250e-01,  8.9762e-01, -9.4137e-01, -3.2958e-01,\n",
       "         -1.6240e+00, -1.0486e+00,  6.9408e-02, -4.3882e-01, -1.1456e+00,\n",
       "         -1.1039e-02, -1.6881e+00, -1.1646e+00, -1.2448e-01,  1.8252e-01,\n",
       "         -6.7326e-02,  6.1808e-01,  8.6855e-01, -6.7853e-01,  1.7331e+00,\n",
       "         -1.0625e+00,  1.9109e+00,  6.1834e-01, -1.8791e+00,  4.0037e-02,\n",
       "         -2.7370e+00, -1.6154e+00,  6.8997e-01,  3.0402e-01, -4.4998e-01,\n",
       "         -5.4297e-01, -6.3057e-01, -1.7799e+00, -1.4489e+00,  7.8791e-01,\n",
       "          5.2911e-01,  1.8837e+00, -1.3037e+00,  6.6944e-02,  1.7244e-01,\n",
       "          2.5313e-01,  6.3827e-03, -7.2337e-01,  6.6610e-01, -1.5466e+00,\n",
       "          7.5577e-01, -5.1946e-01,  1.2880e+00, -3.7216e-01,  5.2288e-02,\n",
       "         -1.0920e-01, -2.7781e+00,  1.0101e+00,  1.1714e+00, -1.0825e+00,\n",
       "          2.5054e+00, -5.1847e-01,  2.0537e-01,  3.2792e-01,  1.6380e+00,\n",
       "          1.8756e-02, -5.3003e-01,  6.7116e-01, -1.1121e+00, -2.1479e-01,\n",
       "          1.5466e-01,  8.4097e-01, -1.3802e+00,  7.3346e-02, -3.0320e-01,\n",
       "         -1.0254e+00,  1.5466e+00, -1.0493e-01, -7.5792e-03, -5.3772e-01,\n",
       "         -5.5732e-01, -7.1620e-01, -5.4344e-03,  1.7087e-01, -2.1651e+00,\n",
       "          1.5770e+00,  1.5812e+00,  2.7904e-01,  1.4233e+00, -1.0279e+00,\n",
       "         -1.2034e+00,  6.2592e-01,  3.6848e-01,  1.0860e+00, -6.2806e-01,\n",
       "         -2.0386e+00, -6.4906e-03,  1.0405e+00, -1.3705e+00, -4.2430e-01,\n",
       "          1.0155e+00,  1.2504e+00, -6.3742e-01, -6.0506e-01,  1.9588e+00,\n",
       "         -8.5743e-01,  1.0920e+00, -9.1878e-01, -2.5306e-01,  2.3007e-02,\n",
       "          7.6306e-01, -6.1121e-01,  6.0716e-01, -3.2253e-01,  7.0683e-01,\n",
       "          1.1839e-01, -3.7536e-01,  5.0918e-01, -3.7653e-01,  4.2606e-01,\n",
       "         -8.1157e-01, -1.1200e+00,  7.2300e-01, -5.2402e-01,  7.3681e-01,\n",
       "         -1.4894e-02, -3.9866e-01, -1.3328e+00, -3.0788e-01,  4.1313e-01,\n",
       "         -4.6281e-01,  8.0675e-01,  3.2384e-01,  2.6199e-01, -2.3897e-01,\n",
       "         -7.8396e-01, -2.7776e-01, -7.1824e-01, -6.3736e-01, -1.0332e+00,\n",
       "         -2.4927e+00,  1.1069e+00, -7.9720e-01,  6.3841e-01,  4.7852e-01,\n",
       "         -4.2624e-01,  1.3887e+00,  2.0003e+00,  1.6857e-01, -1.2454e+00,\n",
       "          3.9114e-01,  9.9533e-02,  1.4565e+00,  1.1466e+00,  6.5470e-03,\n",
       "          5.2380e-01, -1.3485e-01, -2.7761e-03,  4.8290e-01,  3.3850e-01,\n",
       "         -5.2551e-01, -4.9407e-01,  8.3260e-01, -7.7990e-03,  1.0510e+00,\n",
       "          1.5032e+00,  6.7717e-01,  6.1900e-01, -7.0904e-01,  5.6948e-01,\n",
       "          1.3181e+00, -6.8655e-01,  1.2600e+00,  9.8315e-01,  6.6518e-01,\n",
       "         -7.2516e-01,  1.5611e-01,  1.2213e+00, -1.0504e+00, -6.2690e-01,\n",
       "          7.1785e-01,  8.9796e-01,  1.3184e+00, -2.0079e+00, -2.8626e-01,\n",
       "          1.1041e+00, -7.4334e-02, -7.2769e-02, -4.4874e-01, -5.5595e-01,\n",
       "          1.2671e-01,  4.8262e-01, -1.5703e-01, -2.5695e-01, -2.6901e+00,\n",
       "         -4.9246e-01, -4.8109e-01, -2.8097e-01,  4.5399e-01, -4.1690e-01,\n",
       "         -1.5761e+00, -7.7863e-01, -5.0627e-01,  5.0275e-01,  6.6708e-01,\n",
       "         -5.2187e-01,  7.4771e-01,  2.6262e-01,  6.1114e-01,  1.4068e-01,\n",
       "          9.3603e-01,  1.2525e-01,  3.5605e-01,  5.1819e-01,  2.3238e-01,\n",
       "         -3.7220e-01, -1.3330e+00,  2.6079e-01,  1.1374e+00, -1.1513e+00,\n",
       "         -2.3456e+00, -4.0742e-01, -1.0443e+00, -9.3386e-01,  3.1525e-01,\n",
       "         -9.9800e-01,  4.8421e-01,  1.5841e+00, -4.6550e-01,  2.5186e-01,\n",
       "          1.1645e-01, -1.7159e-01, -6.2498e-01,  2.2037e-01,  1.3806e+00,\n",
       "         -9.5899e-01, -4.9436e-01,  2.2270e-01,  1.2193e+00, -2.2257e-02,\n",
       "          1.1362e+00, -1.0800e-01, -9.4078e-01, -1.8301e+00, -7.0390e-02,\n",
       "         -4.1009e-01,  1.4597e-01,  2.5700e+00,  2.1648e-01, -7.4715e-01,\n",
       "          6.3614e-01, -6.3772e-01, -2.0694e-01,  7.3199e-01, -1.2757e+00,\n",
       "         -1.5689e+00, -5.0105e-01, -2.3347e-01, -3.4472e-01,  9.3678e-02,\n",
       "          3.7053e-03,  5.1454e-01, -8.0078e-01,  2.7169e-01,  8.1253e-01,\n",
       "         -1.2468e+00, -9.4475e-02, -5.8314e-02, -8.7377e-01,  1.8062e+00,\n",
       "         -7.8589e-01,  1.9853e+00,  9.8220e-02, -2.3944e-01,  9.5837e-01,\n",
       "         -1.3544e+00,  8.0494e-01,  4.1095e-01, -2.2636e-01, -5.2819e-01,\n",
       "          1.5470e+00,  8.7117e-01,  1.5838e+00,  5.0419e-02, -1.4392e+00,\n",
       "         -8.0345e-01, -1.1815e+00,  7.2029e-01,  8.9423e-01, -3.2848e-01,\n",
       "         -1.7846e-01,  8.8761e-01, -5.6221e-02, -2.6450e+00, -2.7082e+00,\n",
       "         -2.3747e-02, -3.8276e-03,  5.5541e-01,  1.2866e-01,  1.9229e+00,\n",
       "          8.0101e-01,  2.3445e+00, -3.7314e-02, -9.2425e-01,  3.8197e-01,\n",
       "         -7.5706e-01,  7.1494e-01, -3.2390e-01,  1.4819e+00, -4.7292e-01,\n",
       "         -1.1280e+00, -8.0279e-01, -5.6574e-01,  1.5954e+00, -1.1076e+00,\n",
       "          1.0047e+00,  5.4380e-01, -1.6305e-01,  1.2479e+00, -6.1970e-01,\n",
       "          7.4442e-01, -6.7285e-01,  8.1312e-01, -1.4584e+00, -5.7809e-01,\n",
       "         -3.5300e-01,  5.8298e-01, -1.4829e+00, -3.7905e-01, -2.4240e+00,\n",
       "         -1.1404e+00, -1.1981e+00,  3.2999e-01,  2.5034e-01, -6.6396e-01,\n",
       "          7.2110e-01,  7.1421e-01,  8.2685e-01,  1.0217e+00,  4.9552e-01,\n",
       "          6.4677e-01,  2.7809e-01,  6.1560e-01, -1.7110e+00,  1.1738e+00,\n",
       "          4.6067e-01, -5.3849e-01,  2.4118e-02,  1.3164e+00,  4.8944e-01,\n",
       "          3.5332e-01,  5.7008e-01,  6.6632e-01,  5.1346e-01, -1.9218e+00,\n",
       "         -6.0335e-01,  1.3572e+00, -5.2460e-01,  7.7523e-01,  1.1047e+00,\n",
       "         -3.2516e-01, -1.0746e+00,  1.2774e+00,  1.1638e+00, -1.7637e+00,\n",
       "          2.9306e-01,  9.1187e-01, -1.3300e+00, -9.5004e-01, -2.1265e-01,\n",
       "          2.4643e-01, -6.0341e-01, -6.6189e-01,  3.7925e-01, -1.1506e+00,\n",
       "          7.6906e-01,  9.7483e-01,  4.2308e-01, -6.3196e-01, -9.2117e-01,\n",
       "         -2.3037e-01, -1.6651e+00, -2.7394e+00,  1.2060e-02,  1.5540e-01,\n",
       "          7.8749e-01,  3.0226e-01, -9.0351e-02,  5.5662e-01, -1.1995e+00,\n",
       "          8.5861e-01, -7.2669e-01, -2.7717e-01, -1.2880e+00, -4.1194e-01,\n",
       "          2.4240e-01, -2.0504e-01,  1.3056e+00, -1.3999e+00,  5.7564e-01,\n",
       "          8.0062e-01, -1.0132e+00, -1.0364e+00, -3.6181e-02,  9.9960e-01],\n",
       "        [-1.1172e+00, -2.3250e+00,  1.7308e-01, -1.3901e+00,  1.2266e+00,\n",
       "         -3.8104e-01,  6.4923e-01, -2.7353e-01,  1.1112e+00, -1.2734e+00,\n",
       "          2.0568e-01, -5.7846e-01,  1.0194e+00, -4.7045e-01, -6.6693e-01,\n",
       "          6.5577e-01, -8.8998e-01,  6.6775e-01, -1.8153e+00, -4.5096e-01,\n",
       "          1.2342e+00, -1.1740e-01,  5.9059e-01,  1.5234e+00, -1.2357e-01,\n",
       "         -5.7878e-01, -1.0100e+00,  2.9279e-01,  5.4312e-01,  2.6082e+00,\n",
       "          1.2624e+00,  1.2074e+00,  6.5802e-01, -1.3506e-01,  4.5551e-01,\n",
       "          5.5277e-01, -8.3678e-01, -1.0379e+00, -9.5490e-01, -1.9901e-01,\n",
       "         -1.7941e+00,  1.7682e+00,  1.5616e+00,  5.6205e-01,  2.5076e-02,\n",
       "          5.0698e-02, -2.6352e-01, -1.1628e+00,  6.2936e-01, -9.6889e-01,\n",
       "         -1.3069e+00, -1.1954e+00, -4.6086e-01,  4.0328e-01, -2.1607e-01,\n",
       "          2.2387e-02,  1.9158e-01, -9.1989e-01,  2.4321e+00, -1.0478e-01,\n",
       "          1.0378e+00,  9.2674e-01, -8.9149e-01, -1.2546e-01, -4.2536e-01,\n",
       "         -1.0173e+00,  2.3081e-01, -1.6028e+00, -1.6141e+00,  4.1166e-01,\n",
       "         -1.8575e+00, -2.6376e-02, -3.3793e-01, -4.6282e-01, -7.3066e-01,\n",
       "          8.1311e-01, -3.6708e-01,  2.9244e-01,  9.7832e-01, -2.2015e+00,\n",
       "         -4.4604e-01,  1.0856e+00, -2.0739e+00, -9.1971e-02, -6.4501e-01,\n",
       "          5.4974e-01,  7.7607e-01,  1.4091e+00,  6.1737e-01, -1.0319e+00,\n",
       "         -1.7539e+00, -2.7686e+00,  5.1035e-01,  1.6261e+00,  5.7099e-01,\n",
       "          1.2347e+00, -1.1598e+00,  7.2907e-01, -1.7071e-01,  1.2746e+00,\n",
       "         -2.5864e-01, -1.6287e-01, -2.5636e+00,  5.1773e-01,  5.2463e-01,\n",
       "         -7.7810e-01, -1.5134e+00,  1.2508e-01, -8.4432e-01,  2.3494e-01,\n",
       "         -8.3249e-01,  3.4331e-01,  1.4028e-01, -6.0514e-01, -1.1746e+00,\n",
       "          1.6049e-01, -9.0971e-01,  1.0467e+00, -2.4093e-01, -1.6172e+00,\n",
       "          3.8830e-01,  4.6295e-01, -5.3396e-01,  3.6447e+00,  2.3938e-01,\n",
       "         -1.1035e+00,  2.4794e+00, -6.6444e-01,  7.5411e-01,  1.1168e+00,\n",
       "         -4.5836e-01, -2.4396e-01,  3.7148e-01, -1.1725e+00, -3.2655e-01,\n",
       "          5.9989e-01,  2.1952e-01, -1.1563e+00,  1.0566e+00, -2.0088e+00,\n",
       "          1.0297e+00,  1.2066e+00,  1.0981e-01,  2.0101e-01, -5.2604e-01,\n",
       "          7.6277e-01, -1.1209e-01,  7.8752e-01, -1.8257e-01,  9.6633e-01,\n",
       "         -3.7773e-01, -2.6047e-01,  6.1236e-01,  8.2063e-01,  3.1754e-01,\n",
       "         -7.0200e-01, -7.1029e-01, -1.2931e-01, -1.0883e+00, -1.2981e+00,\n",
       "         -3.8457e-01, -5.6439e-01,  4.5003e-01, -2.4318e-01, -1.3333e-01,\n",
       "         -1.6317e+00,  1.7426e+00, -1.9691e-02, -1.1559e-01,  1.1860e+00,\n",
       "          5.8884e-01, -4.9497e-01, -1.2881e+00, -3.0453e-01, -6.6151e-01,\n",
       "         -1.1776e+00,  6.6251e-01, -7.7309e-01,  6.4318e-01,  1.5579e+00,\n",
       "          3.8109e-01, -2.3167e+00, -9.5023e-01, -1.2484e+00, -2.2704e-01,\n",
       "         -1.0516e+00, -2.6648e-01,  8.7873e-01,  9.7022e-01,  1.1433e+00,\n",
       "         -1.1635e+00,  4.1140e-01, -2.2847e+00,  9.1732e-01,  3.2164e-01,\n",
       "          1.7423e-01, -6.6579e-01, -1.1064e-01, -2.0221e+00,  1.0079e+00,\n",
       "          4.0264e-01, -1.0472e+00,  2.6209e-01, -3.9785e-01,  1.5590e+00,\n",
       "          4.8304e-01,  5.3962e-01, -3.0409e-01,  6.5241e-01,  2.7086e-01,\n",
       "          1.2597e+00,  7.4020e-01,  2.8887e-01, -6.4779e-01, -1.0483e+00,\n",
       "         -3.1114e-01,  3.5873e-02,  1.4018e+00, -7.3250e-02,  7.1552e-01,\n",
       "         -2.8331e-01,  5.2950e-01,  1.6102e+00,  5.0113e-01, -1.1002e+00,\n",
       "         -9.4108e-02, -9.5678e-01,  2.4756e+00, -2.6025e-01,  2.9197e+00,\n",
       "         -4.6507e-01,  2.8593e-01,  1.3266e+00, -1.0945e+00, -1.5524e+00,\n",
       "          1.3648e+00, -3.5061e-02,  2.0919e+00, -1.4057e-01,  2.7515e-01,\n",
       "         -1.7223e+00,  5.6115e-01,  1.5430e+00,  9.6494e-01,  1.4780e-01,\n",
       "          8.4586e-01,  2.8295e+00,  2.4764e+00, -2.1817e+00, -2.6322e+00,\n",
       "         -3.3489e-01, -7.0792e-01, -1.0397e+00, -1.2720e+00, -4.7486e-01,\n",
       "          1.0564e+00, -8.5269e-01, -8.1611e-02,  7.0631e-01, -7.2016e-01,\n",
       "         -2.1092e+00,  1.0662e+00, -1.6503e+00, -5.6101e-02, -5.3722e-01,\n",
       "         -8.1758e-02, -2.0106e+00, -2.1410e+00,  9.9471e-01,  5.3516e-01,\n",
       "         -8.0675e-02, -9.1241e-01,  1.0167e+00,  3.8835e-01,  4.9201e-01,\n",
       "         -4.5543e-01, -1.5269e+00, -8.9409e-01,  6.8819e-01, -8.0543e-01,\n",
       "          1.2761e+00, -2.6179e-01,  2.6005e-01,  1.6782e-01, -8.6910e-02,\n",
       "          5.4879e-01,  1.3526e+00, -1.7939e+00,  6.1937e-01, -3.9297e-02,\n",
       "         -9.5557e-01, -7.1079e-02,  4.7073e-01,  3.8774e-01, -7.6592e-01,\n",
       "         -1.0610e+00,  1.1560e-01,  5.8076e-01,  2.5471e+00, -1.4868e-01,\n",
       "          1.8664e+00, -1.6387e+00,  6.8188e-01, -6.6117e-01,  1.1942e-01,\n",
       "          1.0596e+00, -1.5361e+00,  2.3840e-01, -1.6060e+00, -3.3215e-02,\n",
       "         -1.3015e+00,  1.0235e+00,  2.7462e-02, -2.0729e-01, -7.7396e-01,\n",
       "         -6.9871e-01, -2.1373e-01, -5.4603e-01,  5.0577e-01, -4.6222e-01,\n",
       "          5.8398e-01, -4.2303e-01, -1.4339e-01,  1.0629e+00, -1.0010e+00,\n",
       "          3.3343e-01,  6.0701e-01,  4.6395e-01,  2.6943e-01, -1.9492e+00,\n",
       "          8.4985e-01, -1.9708e+00,  1.0123e+00, -4.5568e-01,  1.6998e+00,\n",
       "          1.3917e+00,  8.0424e-01, -3.1025e-01,  7.9004e-01, -3.0232e-01,\n",
       "          5.6153e-01, -5.9307e-01,  6.1997e-01,  1.3142e+00,  6.7724e-01,\n",
       "          5.9112e-01,  1.0517e+00, -8.1780e-01,  3.2677e-03, -1.2988e+00,\n",
       "          3.6060e-01,  1.5073e+00, -4.8702e-01,  3.9882e-01, -1.2849e+00,\n",
       "          5.8251e-02,  5.6605e-01,  1.6138e-01, -1.2768e+00,  8.1271e-02,\n",
       "          6.5013e-01,  2.7931e-01, -1.8706e+00, -1.4658e-02, -4.7353e-01,\n",
       "          1.3032e+00, -2.8904e-02, -1.7231e+00,  1.0751e+00,  1.3681e+00,\n",
       "          1.7508e+00,  2.7549e-01,  1.1720e-01,  1.5731e+00,  1.9739e+00,\n",
       "         -4.7382e-01,  9.4670e-02,  7.4045e-01,  1.1789e+00, -9.1028e-01,\n",
       "          1.2363e+00, -2.6793e-01,  1.3348e+00,  3.5087e-01,  1.9355e-02,\n",
       "          1.0938e+00,  1.6665e+00, -1.8155e+00,  7.1421e-01, -1.2425e+00,\n",
       "          1.6754e-01,  5.1360e-01, -7.7106e-01, -2.0181e-02, -3.1173e-01,\n",
       "         -3.7703e-01, -3.9112e-01, -1.5382e+00,  4.7161e-01, -8.8622e-01,\n",
       "          8.9026e-01,  9.5305e-01,  1.7720e-01, -1.1427e+00, -6.7155e-01,\n",
       "          7.9440e-01,  3.3458e-01, -5.8822e-01, -4.1345e-01,  5.1424e-01,\n",
       "          3.6178e-01,  7.2658e-01,  1.3482e+00, -5.7945e-01, -2.4687e-01,\n",
       "          2.1553e-01,  6.7635e-01,  1.5496e+00, -1.2317e+00,  6.1987e-02,\n",
       "         -9.7577e-02, -5.3596e-01,  4.6893e-01, -4.2761e-01,  2.1427e-01,\n",
       "         -1.0740e+00,  1.2215e+00,  8.3508e-02, -7.5205e-02, -1.9883e+00,\n",
       "          1.3790e+00,  4.5972e-01, -6.6094e-02, -8.1655e-01, -9.2296e-02,\n",
       "         -1.6821e+00,  2.1425e-01,  9.7696e-01, -4.9010e-01, -9.2427e-02,\n",
       "          3.8561e-01,  3.5005e-01,  6.4917e-01,  1.0324e+00,  2.1673e-01,\n",
       "          1.6799e+00,  2.8302e-01, -1.2784e+00, -6.1089e-01, -1.1193e+00,\n",
       "         -3.9351e-01, -2.2012e-01, -3.2063e-01,  2.7551e+00, -4.7038e-01,\n",
       "          4.3600e-01,  3.6129e-02,  3.7283e-01, -1.5233e+00,  1.5663e+00,\n",
       "         -7.0661e-01, -1.0499e+00, -7.0720e-01,  1.1276e-01,  1.4603e-02,\n",
       "         -1.5701e+00,  1.3059e-01, -1.2847e+00, -4.9385e-01,  6.8986e-01,\n",
       "          8.1781e-01,  1.1649e+00, -5.5972e-01, -3.1454e-01, -1.3877e-01,\n",
       "          2.9346e-02, -9.6092e-01,  1.1451e+00, -2.2172e+00,  1.9245e-01,\n",
       "          1.8280e+00,  1.8771e-01,  3.3474e-01,  5.3513e-01, -1.1536e+00,\n",
       "          9.4821e-01,  2.0190e+00, -6.5300e-01,  4.6304e-02, -5.2296e-02,\n",
       "         -8.2596e-01, -1.6401e+00, -1.5533e+00,  4.6311e-02,  1.3792e+00,\n",
       "          2.7709e-01, -6.2731e-01,  1.2086e+00, -1.0578e+00,  1.3894e+00],\n",
       "        [ 1.2964e+00, -4.0219e-01,  2.3744e+00,  2.0576e-01, -7.7744e-01,\n",
       "         -4.2011e-01, -4.5372e-01,  6.3780e-01, -3.6264e-01, -7.8617e-01,\n",
       "         -2.1862e-01, -7.0098e-01, -1.9063e-01,  1.2741e+00,  8.1524e-01,\n",
       "         -1.8844e-01,  5.4424e-01, -4.1670e-01,  1.3925e+00,  1.1482e+00,\n",
       "          1.2583e+00, -5.7311e-01,  9.2668e-01, -1.1013e+00, -8.5907e-01,\n",
       "          1.1969e+00, -1.6744e+00, -1.5710e-01,  7.8689e-01,  2.3268e+00,\n",
       "          9.7064e-01,  1.1305e+00,  7.3061e-01,  5.9537e-02, -3.6502e-01,\n",
       "          1.9316e+00, -1.5872e+00, -1.0024e-01, -5.0757e-01,  4.6784e-01,\n",
       "         -1.5993e+00,  2.2610e+00, -5.8608e-01, -1.3467e+00, -1.0834e+00,\n",
       "         -2.3944e-01,  1.1062e+00, -4.1006e-01, -2.7578e-01,  4.7330e-01,\n",
       "         -1.6025e+00, -2.8437e-01,  2.1814e-01,  1.2695e+00,  1.6354e+00,\n",
       "          8.0858e-02,  1.9395e-01,  1.0106e+00, -1.7003e+00, -1.0081e+00,\n",
       "         -7.6619e-01,  1.6848e-01,  5.9969e-01,  6.5320e-01, -3.6592e-01,\n",
       "          1.7291e+00,  3.7038e-01, -1.9306e+00, -9.3891e-01,  2.5836e-01,\n",
       "         -5.4245e-01,  6.2439e-02,  8.1406e-01,  1.9272e+00,  1.2346e-01,\n",
       "         -2.1532e+00, -8.0087e-01, -1.3052e+00, -9.4753e-01,  2.1946e+00,\n",
       "         -4.3305e-01,  1.1629e+00, -7.8028e-01,  7.4107e-01,  1.0729e+00,\n",
       "          6.3295e-01,  3.7313e-01,  6.1879e-01,  1.0082e-01, -1.9561e+00,\n",
       "         -1.6788e+00, -1.6493e+00,  1.7346e+00, -1.8335e-01,  1.4552e+00,\n",
       "         -9.0878e-01, -1.7953e-01, -1.3016e-01, -1.1846e+00,  7.4393e-01,\n",
       "         -6.9046e-01, -4.1909e-01, -1.2810e+00,  4.1615e-01,  1.8006e+00,\n",
       "          2.3207e+00, -2.8723e+00, -6.4861e-01, -3.4419e-01,  1.2220e+00,\n",
       "          2.2345e+00, -8.3238e-02,  6.2289e-01,  1.1517e+00,  9.6324e-01,\n",
       "          1.0028e+00,  5.0661e-01,  7.0390e-01,  1.3498e-01,  6.1941e-01,\n",
       "          6.2179e-01, -9.9968e-01,  6.7421e-01, -2.9599e-01,  9.1170e-02,\n",
       "          6.6060e-01, -3.1487e-01,  3.9250e-01, -1.2772e+00,  3.7520e-01,\n",
       "         -5.4370e-01, -6.6259e-01,  9.9463e-01, -5.4682e-01, -1.0825e+00,\n",
       "          2.8279e-01,  3.1266e-01,  7.3702e-02, -6.5135e-01,  2.1814e+00,\n",
       "         -4.5045e-01, -9.7228e-01,  8.4500e-02,  1.9501e+00,  2.7798e-01,\n",
       "         -2.4650e+00,  1.9105e+00, -8.8060e-01, -3.6744e-01, -1.1595e+00,\n",
       "         -8.1431e-01,  5.9140e-01, -7.3037e-01,  5.3516e-02, -1.4998e+00,\n",
       "         -1.0769e+00, -7.7747e-01, -1.2870e-01,  1.2327e-01,  2.5698e-01,\n",
       "          1.2299e+00,  9.9038e-01, -2.5069e-01,  1.4464e+00, -9.6074e-01,\n",
       "         -1.4734e+00,  1.1722e+00, -6.1096e-02, -4.6154e-01,  5.0623e-01,\n",
       "         -1.1124e+00,  1.6515e+00,  1.0247e+00,  1.3162e+00, -1.5622e+00,\n",
       "          1.0175e-01, -6.9530e-01,  1.0328e+00, -1.3260e-01,  1.0319e-01,\n",
       "         -3.4046e-01, -2.1570e+00,  6.6342e-01,  7.2654e-01,  1.2173e-01,\n",
       "         -1.0489e+00,  2.1942e-01,  1.2743e-01,  1.6087e+00,  2.3558e-01,\n",
       "         -1.8680e-01,  1.6618e+00, -1.6344e+00,  1.0677e+00, -6.6900e-01,\n",
       "          8.2327e-01,  3.6629e-01, -7.1203e-01, -1.4715e-02, -1.1641e-01,\n",
       "         -1.1863e+00, -1.2708e-01, -1.9572e+00,  4.7157e-01,  2.7259e-01,\n",
       "          3.1775e-01, -4.3019e-01,  2.8182e+00, -1.3495e-01, -1.3859e+00,\n",
       "         -1.4314e+00,  1.7469e-01, -9.9510e-01,  2.8358e-01, -1.0380e+00,\n",
       "          2.9506e-01, -2.8252e-01,  2.0371e-01, -3.1776e-01, -2.3221e-01,\n",
       "          8.7237e-01, -2.6399e+00, -1.0190e+00, -1.6953e+00, -7.0428e-01,\n",
       "         -2.0515e-01,  8.0685e-01,  6.4598e-01, -6.5617e-01,  1.8758e-01,\n",
       "         -1.5358e+00,  1.3068e+00, -1.9634e-01,  5.9661e-01,  8.4216e-01,\n",
       "         -1.4657e+00,  7.1862e-01,  5.2774e-01,  9.0224e-01,  2.2244e+00,\n",
       "          1.1887e+00,  5.3029e-01,  1.6564e-01, -4.6337e-01,  1.1586e+00,\n",
       "          4.2977e-01, -1.2975e+00,  2.7743e-01,  1.9076e+00,  7.8031e-01,\n",
       "          1.0927e+00,  7.5197e-01,  2.2384e-01,  8.5929e-02, -1.2100e+00,\n",
       "          2.9722e-01,  9.8427e-01,  1.0973e+00, -7.7374e-01, -6.2701e-01,\n",
       "          1.1946e+00,  6.7735e-02,  6.3736e-03,  5.0670e-01, -4.5601e-01,\n",
       "          1.8929e+00, -7.8675e-01,  1.7376e-01, -4.5096e-01,  2.9972e-01,\n",
       "         -6.3187e-01,  2.1009e-01, -1.5068e+00,  7.0336e-01,  1.2741e+00,\n",
       "          5.7232e-01,  1.0291e-01, -5.8101e-01,  1.3824e+00, -1.1709e+00,\n",
       "          4.4739e-01, -8.5068e-01, -3.8050e-01,  9.8239e-02, -1.5006e-01,\n",
       "         -1.8118e+00, -7.3435e-01, -1.5109e+00,  1.7824e-01, -1.0658e+00,\n",
       "          1.0217e+00, -2.6867e-01, -3.8478e-01,  1.3884e+00, -2.8147e-01,\n",
       "         -4.5143e-01,  9.1059e-02,  1.0145e+00, -1.1636e+00, -4.9677e-01,\n",
       "         -7.9105e-01, -1.2754e+00,  2.0459e-01, -1.0013e+00, -1.1233e+00,\n",
       "         -5.9217e-01, -4.0238e-01, -1.0055e+00,  3.5201e-01, -4.0073e-02,\n",
       "         -2.0913e+00, -4.5238e-01, -5.8356e-01, -4.5792e-01, -5.0480e-01,\n",
       "         -8.8944e-01,  2.7934e-01,  1.1140e+00, -8.6527e-02, -6.9748e-01,\n",
       "          9.3337e-02, -5.1738e-01, -3.9919e-01,  1.0069e+00, -1.5203e-01,\n",
       "          1.5061e+00, -1.1207e+00,  7.4036e-01, -1.9716e-02,  6.2486e-01,\n",
       "          3.1399e-01,  3.3544e-01,  2.6328e-01, -9.1158e-01,  1.6037e+00,\n",
       "         -1.2727e+00,  6.9264e-02, -4.7820e-01, -1.3750e+00, -2.1401e-01,\n",
       "          1.0799e+00, -1.5377e+00, -7.5868e-01,  1.9394e-01,  7.4389e-02,\n",
       "         -2.8086e-01,  1.5324e+00,  4.7559e-01,  4.3201e-01, -3.7985e-01,\n",
       "          1.0753e+00, -6.6883e-01,  4.6219e-01, -3.0249e-02, -1.1488e+00,\n",
       "         -1.8580e-02,  1.1368e+00, -5.1558e-01,  4.2709e-01,  5.7955e-01,\n",
       "         -1.2801e-02,  8.1315e-01,  7.1370e-01,  9.5604e-01, -3.3463e-01,\n",
       "          1.1604e+00, -2.7698e-01, -1.1434e+00,  1.4212e-01, -1.5112e-01,\n",
       "          6.8934e-01,  7.9373e-01,  1.1863e+00, -1.0509e+00, -2.9220e-01,\n",
       "         -5.4073e-01, -2.1529e+00,  6.5214e-01, -1.1236e+00, -1.5229e+00,\n",
       "          3.9992e-02,  2.6586e-01, -1.1126e+00,  2.1034e-01,  1.2975e-01,\n",
       "         -1.7070e+00, -3.3040e-01,  5.3745e-01, -8.0323e-01,  3.8758e-01,\n",
       "          3.8793e-01,  4.9947e-02, -7.7425e-02,  3.2644e-01,  7.0165e-01,\n",
       "         -9.2082e-01,  1.1977e+00, -9.5745e-01, -1.2381e+00, -1.2938e+00,\n",
       "          4.9210e-01, -2.3588e-01,  2.0970e-01,  3.8522e-01, -1.3715e+00,\n",
       "         -3.8229e-02,  2.5629e-01, -5.1903e-01, -9.3337e-01,  1.2880e+00,\n",
       "          1.2203e-01,  1.0390e+00,  1.2370e+00,  9.5106e-01, -2.0110e-01,\n",
       "          5.8838e-02,  1.5734e+00,  1.7722e-01,  1.7614e+00,  9.7700e-01,\n",
       "          7.7961e-01,  7.3449e-01,  6.7217e-01,  1.9188e+00,  8.8079e-01,\n",
       "          1.0237e-01,  1.0762e-01, -3.6432e-01,  8.6412e-01, -2.2551e+00,\n",
       "         -1.9105e+00,  6.4244e-01, -8.1204e-01, -7.4116e-02,  1.7280e-01,\n",
       "          7.7140e-01,  1.1823e-01, -1.2366e+00,  4.4586e-01,  1.0521e+00,\n",
       "          1.1482e+00,  1.9647e-01, -7.1625e-01,  7.0080e-01,  2.0159e-01,\n",
       "         -1.0564e+00, -4.0440e-01, -1.2162e-02, -1.8719e+00, -3.0033e-01,\n",
       "          7.5731e-01, -2.9772e-01, -1.4381e+00,  2.5874e+00, -5.8702e-01,\n",
       "          7.8210e-01,  8.4594e-01,  5.3880e-01,  1.2246e+00,  3.1519e-02,\n",
       "          1.0317e-01,  6.4965e-01, -5.2364e-01, -2.9457e-01, -2.5460e+00,\n",
       "         -2.4515e-01, -1.1108e+00, -5.7439e-01,  2.1339e+00,  5.0824e-01,\n",
       "          4.0742e-01,  6.8308e-01,  1.4036e+00, -1.0310e-01, -2.5473e+00,\n",
       "         -1.1608e+00, -1.2384e+00, -1.1157e-01,  3.8409e-01,  2.9988e-01,\n",
       "          1.8234e+00, -1.0756e+00,  1.0356e+00,  1.3145e+00, -3.2057e-01,\n",
       "          1.1040e+00, -5.0082e-01,  2.9355e-02,  5.5840e-01, -1.9102e-02,\n",
       "          5.4309e-01,  2.6348e+00,  9.1409e-01, -8.4522e-01,  9.5912e-01,\n",
       "         -2.6169e-01,  2.1621e+00, -8.0657e-01, -1.8476e+00,  6.1016e-01],\n",
       "        [ 1.5445e-01,  1.7311e+00,  1.4563e+00, -1.0188e-01, -2.7806e-02,\n",
       "          3.5937e-02, -1.2299e+00,  6.9148e-01, -5.0770e-01,  1.1227e+00,\n",
       "         -1.0950e-01,  9.8958e-02, -5.3236e-01,  8.5507e-01,  1.2506e+00,\n",
       "          9.3263e-01, -2.3018e-01, -8.4858e-01, -1.7625e-01, -6.8487e-01,\n",
       "          9.7399e-01,  3.6060e-02,  8.6704e-01, -4.2362e-02, -4.1401e-01,\n",
       "          7.5642e-02,  8.0752e-01, -3.6443e-01, -1.8017e+00,  9.2797e-03,\n",
       "          4.2177e-03,  8.4968e-01, -7.2856e-01,  1.0927e+00, -4.5712e-01,\n",
       "         -1.5749e-01,  4.2330e-01, -7.5643e-01,  8.2925e-01,  9.5727e-02,\n",
       "         -1.9069e-01,  4.3885e-01,  1.1144e+00, -2.9362e-02, -9.8924e-01,\n",
       "          6.4320e-01, -9.4994e-01,  2.5329e+00, -1.7432e-01, -2.1493e+00,\n",
       "         -3.7295e-01, -9.4399e-01,  4.1084e-01,  1.8903e-01,  3.3576e-01,\n",
       "         -1.0957e+00, -6.5571e-03, -2.2682e-01, -6.8618e-01,  1.2113e+00,\n",
       "          1.4909e+00, -6.1577e-01,  1.0209e+00, -1.0674e+00, -1.3458e+00,\n",
       "         -2.1805e-02, -1.8816e-01,  1.6651e-01,  9.0305e-01, -1.3002e+00,\n",
       "          2.8717e-01, -1.3116e+00,  1.2731e+00,  1.0713e+00,  7.7674e-01,\n",
       "          1.1224e+00,  2.2157e-02, -8.3520e-01,  4.8939e-01, -8.2463e-01,\n",
       "         -1.7696e+00,  6.2741e-01, -2.6116e-01, -1.4531e+00, -1.7658e-01,\n",
       "          4.6143e-01,  1.4532e+00, -9.6810e-02,  4.0659e+00,  9.9810e-01,\n",
       "         -2.3764e-01, -1.3496e-01,  3.3330e-02, -6.1550e-01, -1.3864e-02,\n",
       "         -1.1635e+00, -2.3165e-01,  1.4745e-01, -3.1703e-01,  8.1336e-01,\n",
       "         -9.8383e-01, -1.4483e+00, -3.7177e-01,  2.0775e-01, -1.8449e+00,\n",
       "          9.5346e-01,  2.4340e+00,  2.8261e-01, -1.2676e+00, -1.2836e+00,\n",
       "          1.5245e+00, -1.1262e+00,  9.1921e-01,  7.9615e-01,  1.0223e+00,\n",
       "          1.0095e+00, -3.8941e-01, -5.0272e-01, -7.8744e-01, -3.8408e-01,\n",
       "          1.1041e+00,  1.6302e+00,  4.7108e-01,  1.0709e+00,  1.0939e-01,\n",
       "          1.6135e+00, -1.2721e+00,  1.2875e+00,  3.1524e-01, -2.9073e-01,\n",
       "         -3.2507e-01, -1.8697e-01,  7.3807e-01, -9.0851e-01,  4.6672e-01,\n",
       "          8.1526e-01, -1.2284e+00,  4.6615e-01, -2.4875e-01,  1.4591e+00,\n",
       "         -1.2451e+00,  6.0578e-01,  1.3779e+00, -6.0779e-02,  5.7554e-01,\n",
       "         -2.4876e-01, -3.1659e-01,  8.0372e-01, -8.3142e-01,  6.2235e-01,\n",
       "          4.1588e-01, -5.2213e-01,  3.9797e-01,  9.0851e-01, -3.9902e-01,\n",
       "         -1.4961e+00, -5.9404e-01,  2.6230e-01, -4.0275e-01,  1.4309e+00,\n",
       "          7.3177e-01,  1.1052e+00, -7.7861e-01,  2.3199e-02, -1.1746e-01,\n",
       "         -1.5235e+00,  1.5818e-01, -1.9672e+00, -1.4794e+00, -1.1510e+00,\n",
       "          2.7508e-02, -1.3392e+00, -7.7544e-01, -3.6528e-01, -2.4331e-01,\n",
       "         -1.3330e+00, -2.8126e-01,  6.5119e-01,  1.2447e+00, -1.0725e+00,\n",
       "         -7.0047e-01, -1.0814e+00, -1.1894e-01,  2.8618e-02, -1.3495e+00,\n",
       "         -1.7838e-01,  1.9498e-01,  3.2054e-01,  2.0321e-01,  1.6912e+00,\n",
       "          6.7198e-01,  6.0399e-01,  4.9825e-01, -1.9173e+00, -1.0506e+00,\n",
       "          7.8326e-01, -1.2833e-01, -2.9265e+00,  1.4996e-01,  4.3232e-01,\n",
       "          7.0075e-01, -9.9580e-02, -4.2421e-01, -1.2928e-01, -1.8555e+00,\n",
       "          9.5107e-01,  2.5526e+00, -4.5446e-01,  8.2196e-01, -1.6166e+00,\n",
       "          7.7672e-02, -7.9175e-02, -1.4390e+00,  4.8716e-01, -1.2014e+00,\n",
       "          2.6335e-01,  5.8762e-01,  1.3897e+00,  1.6205e+00,  2.3340e-01,\n",
       "          8.5461e-01,  2.5366e-01, -5.1517e-01,  1.9344e+00, -1.0954e+00,\n",
       "         -2.2068e-01,  1.0900e+00, -6.4738e-01,  1.5665e+00,  2.2256e-01,\n",
       "         -5.2129e-02,  3.1294e+00,  2.6269e-01,  4.8429e-01,  2.5880e-02,\n",
       "         -1.9481e-01, -1.2777e+00, -2.2532e-01,  1.0051e+00,  6.6020e-01,\n",
       "         -1.0943e+00, -1.0437e+00, -6.5003e-01, -1.0045e-01,  7.2010e-01,\n",
       "         -1.1164e+00,  1.0153e+00,  1.5916e-01,  1.9832e+00,  8.1839e-02,\n",
       "         -5.4903e-01,  8.1275e-01,  4.4051e-01, -1.1385e-01, -5.1343e-01,\n",
       "         -5.2575e-01, -1.7558e+00, -1.8238e-01,  1.0218e+00,  7.1826e-01,\n",
       "         -1.2709e+00, -1.0996e+00,  1.8678e+00,  1.4483e-01,  2.0544e-02,\n",
       "          4.5769e-01,  1.3877e+00,  1.4976e+00,  8.9189e-02, -1.4644e+00,\n",
       "          1.2757e-01,  9.6022e-02,  5.8641e-01,  3.7533e-01, -3.3949e-01,\n",
       "         -1.0309e+00,  4.0864e-01,  3.6751e-01, -4.3543e-01, -9.7507e-01,\n",
       "          5.5511e-01, -6.7302e-01,  1.0106e+00, -1.2157e+00, -3.5357e-01,\n",
       "         -3.4890e-01, -1.4904e+00,  6.6771e-01, -5.3020e-02,  3.8304e-01,\n",
       "          8.9679e-01,  4.2440e-01,  1.9669e-01,  1.8544e+00,  9.7177e-01,\n",
       "         -2.6951e-01,  2.0231e-02, -5.5447e-01, -6.6835e-01,  1.7633e+00,\n",
       "         -1.2607e+00,  2.5165e-01,  3.6466e-01,  2.1693e+00, -7.1311e-02,\n",
       "         -1.6254e+00, -4.4487e-01,  4.1558e-01,  1.6121e-01, -1.1579e+00,\n",
       "          6.8316e-02,  2.3468e+00,  2.1358e+00, -1.4312e+00, -1.7484e-01,\n",
       "          1.1263e-01, -1.7708e-01,  9.7753e-01,  6.7364e-01,  7.8784e-01,\n",
       "         -2.3569e-01, -1.4564e+00, -1.7057e-01,  1.9242e-01,  7.3295e-01,\n",
       "         -3.8493e-01,  5.8666e-01,  8.3217e-01,  4.5194e-01,  1.4030e+00,\n",
       "         -1.0177e+00,  1.5109e+00,  2.7321e-01, -4.0225e-02, -7.7426e-01,\n",
       "         -6.7665e-01,  1.7864e+00, -2.2869e-01,  2.0304e-01, -6.0943e-01,\n",
       "         -7.3756e-01,  2.1536e+00, -7.7851e-01,  9.3094e-01,  2.2953e+00,\n",
       "          2.7084e-01, -1.2423e-01,  2.2379e-01,  1.6692e+00, -1.0774e+00,\n",
       "         -9.4664e-01, -1.9704e+00,  2.4090e-01, -1.0850e+00, -2.1909e-01,\n",
       "         -1.6972e+00, -9.8495e-01, -1.5130e+00,  4.3498e-01,  5.0008e-01,\n",
       "         -3.4929e-01, -1.3764e+00,  1.5784e+00, -3.2375e-01, -1.1340e+00,\n",
       "          1.0732e+00,  4.6714e-01, -2.3353e-01,  2.3807e-01, -1.1517e+00,\n",
       "          1.4285e+00, -1.3971e+00, -7.4938e-01, -2.7284e+00,  8.1934e-01,\n",
       "          3.1012e-01, -5.5331e-01,  3.5371e-01,  2.5274e+00,  6.6050e-02,\n",
       "          7.9548e-01, -1.8505e+00, -2.1725e-01, -6.8744e-01, -5.5565e-01,\n",
       "         -9.3559e-01, -1.2166e+00,  1.3619e+00,  4.7237e-01,  1.2927e-01,\n",
       "          1.2420e+00, -1.3414e-01,  6.4378e-01, -3.8690e-01,  1.5900e+00,\n",
       "         -5.5407e-02,  1.0140e+00,  3.7851e-01,  1.0176e+00,  1.9647e+00,\n",
       "          1.8875e-01, -2.3880e+00,  1.4718e+00,  1.0299e+00,  2.1775e-01,\n",
       "          8.1816e-01,  2.6079e-02, -1.7996e+00,  1.2654e-01,  4.3718e-02,\n",
       "         -3.7277e-01,  1.0454e+00,  8.0923e-01, -5.4380e-02, -6.6104e-01,\n",
       "         -5.8441e-01, -1.9398e-01,  4.7718e-01,  1.1934e+00, -5.0087e-01,\n",
       "          1.5142e+00, -2.0247e+00,  5.2932e-01, -3.5698e-01, -1.0597e-01,\n",
       "         -5.9390e-01,  5.4595e-01, -3.4759e-01,  1.1678e+00,  6.0782e-01,\n",
       "          9.9070e-01,  2.2673e-01,  1.2439e-01,  4.4942e-01,  4.0619e-02,\n",
       "          1.5246e-01,  1.1003e-01, -1.8581e+00, -1.0308e+00, -1.7988e-01,\n",
       "          8.1798e-01,  1.7178e+00, -2.8712e-01, -1.6958e-01, -1.3541e+00,\n",
       "         -1.4834e+00, -1.9212e+00,  7.6171e-01, -1.8117e+00, -3.4663e-01,\n",
       "          1.1857e+00,  4.0068e-01,  2.4968e+00, -8.6639e-01, -4.9533e-01,\n",
       "         -1.9044e+00, -1.5141e-01,  8.2502e-01,  3.1098e+00, -7.7249e-01,\n",
       "         -1.5466e+00, -1.5003e+00,  1.0376e+00,  7.3843e-03,  8.0927e-01,\n",
       "          3.9490e-01, -4.8948e-01, -6.8030e-01,  1.3961e+00,  2.0930e+00,\n",
       "         -1.0443e-01,  5.2360e-01, -1.2487e+00,  7.5788e-01,  5.5163e-01,\n",
       "          4.9274e-01,  1.3468e-01, -1.1343e+00,  5.0072e-01, -7.1616e-01,\n",
       "         -1.2654e+00, -3.9870e-02, -1.6195e-02,  4.1301e-01, -3.4303e-01,\n",
       "          3.7970e-01,  1.9290e+00, -3.0796e-01, -3.0523e-01,  1.1695e-01,\n",
       "         -1.1149e+00,  1.1375e+00, -1.0584e+00, -4.0021e-01,  1.4796e-01,\n",
       "         -9.0097e-01,  6.1774e-01, -3.6977e-01,  6.6389e-01,  2.5862e-02],\n",
       "        [ 1.2732e+00,  1.0355e+00, -1.1843e+00,  2.1289e-01,  4.0236e-01,\n",
       "          9.4885e-01, -1.2619e+00,  6.3421e-01,  8.7758e-01, -8.0129e-01,\n",
       "          1.0042e+00, -4.9869e-01, -7.6598e-01, -7.8142e-01, -1.4445e+00,\n",
       "          9.6321e-01, -1.8705e-01, -4.4532e-01, -1.3229e+00, -3.0912e-01,\n",
       "          3.0080e-01, -6.1306e-02, -1.4550e+00,  1.6342e+00, -2.7435e+00,\n",
       "          6.9196e-02, -1.4115e+00,  8.1871e-01,  8.8555e-01,  3.7606e-01,\n",
       "          7.9048e-02,  7.2700e-01,  6.8633e-01, -1.5092e+00, -1.0812e+00,\n",
       "          1.4546e+00,  1.2588e+00,  1.2086e+00, -1.0959e+00,  8.7826e-01,\n",
       "          4.0462e-01, -5.2720e-01, -1.1372e+00, -4.1889e-01,  1.3302e+00,\n",
       "          5.8705e-01,  2.9691e-02,  5.6844e-01, -2.0065e-01, -5.3294e-01,\n",
       "         -2.6723e-02,  9.9261e-01,  2.4566e-01,  2.7659e-01,  8.8085e-01,\n",
       "          8.6580e-01,  6.8834e-01, -1.8838e-01, -1.9434e+00, -1.0390e+00,\n",
       "         -5.5823e-01, -4.0561e-01,  1.7778e-01,  9.3747e-01,  1.4835e+00,\n",
       "         -1.1908e+00, -1.2898e+00,  1.2852e+00,  2.4326e-01,  9.4675e-01,\n",
       "         -4.6730e-01, -1.2318e+00, -1.3760e-01, -1.8708e+00, -1.2594e+00,\n",
       "          5.2040e-02,  1.7784e-01, -4.4206e-02, -1.3166e-01,  2.0189e-01,\n",
       "         -1.3329e+00,  8.0015e-01, -8.7673e-01, -7.9503e-03,  6.2641e-01,\n",
       "         -5.9655e-01, -4.1934e-01,  3.4392e-01,  1.2605e+00, -1.0953e+00,\n",
       "         -1.1007e+00, -2.1422e-01,  1.2313e+00, -1.5891e-01, -1.2133e+00,\n",
       "          1.7867e+00, -1.5200e-01, -1.2986e-01,  6.0155e-01, -9.1655e-01,\n",
       "          1.9446e+00, -2.3777e+00, -5.4600e-01,  3.0637e-01,  6.2285e-01,\n",
       "         -1.2671e-01,  3.2692e-02, -1.1627e-01, -6.4761e-01, -4.4674e-01,\n",
       "         -1.6011e+00, -5.6648e-01,  4.4862e-01, -5.9655e-01, -9.9190e-01,\n",
       "          5.9804e-01,  1.0726e+00,  2.2562e+00, -1.3196e+00,  1.6140e+00,\n",
       "          4.3827e-01, -1.1447e-01, -7.6226e-01, -8.7319e-01,  1.3803e-01,\n",
       "         -5.2910e-01,  4.5626e-01, -8.3609e-01, -1.6322e+00,  2.5615e+00,\n",
       "          9.3319e-01, -1.1856e+00, -9.9265e-01, -6.7616e-01,  1.2456e+00,\n",
       "         -1.4434e+00,  4.5825e-01, -1.1994e+00,  4.9294e-02,  2.3868e-01,\n",
       "          1.5798e+00, -1.3767e+00, -1.8975e+00,  3.9109e-01,  4.4517e-01,\n",
       "         -1.3443e+00, -7.4750e-01,  1.9107e+00,  2.5065e+00, -6.9460e-01,\n",
       "          1.9006e+00, -6.1676e-01, -6.6508e-01, -5.0244e-01, -1.0732e+00,\n",
       "         -1.2375e-01, -8.8049e-01, -6.5242e-01,  7.4747e-01, -1.1626e+00,\n",
       "         -2.8120e-01,  1.5252e-01, -3.8888e-02,  1.2980e+00,  1.0628e-01,\n",
       "         -5.4573e-02,  1.0566e+00, -9.9121e-01,  3.6580e-01,  3.6778e-01,\n",
       "         -1.1653e-01,  5.6119e-01,  1.1483e+00, -6.0939e-01, -1.4377e+00,\n",
       "          4.1865e-01,  1.2985e+00,  1.0124e-01,  5.5269e-01,  1.3216e+00,\n",
       "         -1.4975e+00,  2.6343e-01, -4.1181e-01, -1.0155e+00, -9.8288e-01,\n",
       "         -3.2673e-01, -8.7965e-01,  7.3367e-01, -7.2220e-01, -1.0247e+00,\n",
       "         -1.0423e-01,  5.8274e-01,  6.8161e-01, -1.0257e+00,  1.5258e+00,\n",
       "         -5.0911e-02, -3.1374e+00,  1.0275e+00, -1.4450e-01, -4.6540e-01,\n",
       "         -1.7065e+00, -2.0273e-01, -9.1125e-02, -1.6194e+00,  7.7205e-01,\n",
       "         -5.2785e-01,  3.0340e-01,  3.9502e-01, -7.7803e-01, -1.8550e+00,\n",
       "         -4.5232e-01, -2.7459e-01, -1.3360e+00,  1.2602e+00,  6.6497e-01,\n",
       "         -1.0108e+00,  8.4048e-01,  1.0028e+00,  2.3721e-01,  1.4220e+00,\n",
       "         -1.7559e+00,  4.1587e-01,  7.8536e-01,  1.3954e+00, -7.2876e-01,\n",
       "         -2.3357e+00, -1.1040e+00, -3.9781e-02, -1.1895e+00, -1.6752e+00,\n",
       "          7.8727e-01, -3.8626e+00,  2.3110e-01, -8.4193e-02,  2.9582e-02,\n",
       "         -2.3067e+00,  1.8677e+00, -6.4348e-02,  7.2638e-02,  2.0734e-01,\n",
       "          7.7731e-01,  4.7860e-01, -7.5275e-01,  3.5756e-01, -4.9493e-01,\n",
       "         -5.3297e-01, -1.0219e+00,  1.5194e+00,  6.0216e-01,  1.2488e-02,\n",
       "         -1.3593e+00, -6.8739e-01,  1.2814e+00, -3.5707e-01, -9.5533e-01,\n",
       "          5.7101e-02, -1.0027e+00,  3.6140e-01,  2.0985e-02, -2.9628e-01,\n",
       "         -1.2389e+00, -5.0693e-01, -1.3507e+00,  5.8786e-01,  1.2226e+00,\n",
       "         -4.7674e-01,  1.7119e-01, -4.8344e-01,  4.2999e-01, -1.0036e+00,\n",
       "          2.3166e-01,  3.8086e-01,  5.7527e-01,  1.0477e-01,  1.6822e+00,\n",
       "         -2.3335e-01, -1.2121e+00,  1.1944e+00, -2.2273e+00,  1.8328e-01,\n",
       "         -7.2198e-01,  5.0176e-01, -3.4680e-01, -6.3924e-01, -4.5526e-01,\n",
       "          1.5825e-01,  7.3057e-02,  1.0931e+00,  4.9374e-02,  6.3976e-01,\n",
       "          5.6453e-01, -1.0432e+00,  1.1248e+00,  6.3668e-01, -2.4241e-01,\n",
       "         -6.3991e-01, -1.3600e-01,  1.6652e-01, -4.2948e-01,  6.2474e-01,\n",
       "          1.1249e+00, -8.4674e-01, -1.3907e+00, -5.4255e-02,  7.6552e-01,\n",
       "          9.8987e-01, -6.2390e-02, -7.2610e-01, -2.7181e-01,  4.7421e-01,\n",
       "          6.1037e-01, -9.8575e-01,  5.8613e-01, -5.9681e-01, -2.7532e-01,\n",
       "          1.1108e+00, -2.4292e+00, -5.0240e-01,  2.2136e-01, -9.1195e-01,\n",
       "         -6.4095e-01, -7.2660e-01, -2.0164e-01, -1.0933e+00,  1.1143e+00,\n",
       "         -5.8338e-01,  4.5574e-02,  6.5458e-01,  2.3892e-01, -3.7923e-01,\n",
       "          5.8072e-01, -1.4418e+00, -1.2930e+00, -5.5240e-01,  5.9966e-01,\n",
       "         -2.0196e-01,  4.9112e-01,  3.7180e-01, -1.0766e+00,  4.6598e-02,\n",
       "          3.7848e-01,  1.8129e+00,  7.0624e-01,  3.4328e-01,  1.6152e+00,\n",
       "          2.2739e-01, -3.6652e-01,  5.1276e-02,  4.7468e-01, -4.9452e-01,\n",
       "         -7.4014e-02, -5.1839e-01,  6.7433e-01,  6.8024e-01,  9.7629e-02,\n",
       "          5.2730e-01,  4.6530e-01,  3.1717e-01, -2.4248e+00,  3.0957e-02,\n",
       "         -2.4564e-01,  1.0249e+00, -8.1806e-01,  2.2943e-01,  3.1364e-01,\n",
       "         -8.8993e-01,  6.2366e-03, -1.9995e+00, -1.0311e+00, -2.8315e+00,\n",
       "         -1.0627e+00,  1.1484e+00, -3.1322e-01,  9.4274e-01, -5.2492e-01,\n",
       "         -9.2432e-01, -4.8877e-01, -5.2907e-01, -1.5711e+00, -1.1941e+00,\n",
       "         -1.8461e-01, -8.5426e-01, -5.3063e-01,  5.5836e-02,  4.5859e-01,\n",
       "         -1.0485e+00, -3.3910e-01,  2.0799e-01,  4.8282e-01,  1.9755e+00,\n",
       "          7.9444e-01, -1.4533e+00, -2.8347e-01, -6.0088e-01, -1.4454e+00,\n",
       "         -3.8528e-01,  4.9248e-02, -1.6803e-01,  4.3232e-01, -1.1899e+00,\n",
       "          9.3755e-01,  6.1482e-01, -9.2205e-01,  7.7378e-01,  1.4935e-01,\n",
       "          1.6424e+00, -5.0223e-01,  4.5138e-02, -2.9984e-01, -1.1839e+00,\n",
       "         -1.6776e+00, -5.2006e-01, -1.0447e+00,  1.8359e+00, -2.9249e-01,\n",
       "         -9.9344e-01, -6.3983e-01,  8.3249e-01,  1.9604e-01,  1.8111e-01,\n",
       "         -7.0327e-01, -2.4338e-01,  6.8376e-01,  1.5447e+00, -1.1455e+00,\n",
       "         -6.4406e-01,  2.8553e-01,  2.4030e+00, -6.0826e-01, -1.1496e+00,\n",
       "          1.6597e-01,  4.6180e-01,  1.8674e-01,  2.5320e+00, -4.9230e-02,\n",
       "         -8.3230e-01,  7.7441e-02, -2.1292e+00,  1.7944e-01, -2.3933e-02,\n",
       "         -1.7381e+00,  1.4644e+00, -2.9163e-01,  8.5096e-01,  3.1509e-01,\n",
       "         -3.3721e-01,  4.9367e-01,  9.8349e-01,  5.4051e-01,  2.3311e-01,\n",
       "         -1.3192e-01,  9.5938e-02, -1.9960e-01,  3.1919e-01, -4.8571e-01,\n",
       "          3.4023e-02, -1.3027e+00,  1.8064e-01,  3.4833e-01,  4.4771e-01,\n",
       "          1.1581e+00, -4.9684e-02, -1.8105e+00, -7.4182e-01,  2.9604e-02,\n",
       "          7.5783e-01,  3.6254e-02,  2.4236e-01, -3.2308e-01,  2.1674e-01,\n",
       "          2.0248e-02, -9.0274e-01,  1.3701e+00, -6.1570e-01,  2.6643e+00,\n",
       "         -2.1901e-01,  7.6118e-01,  1.6272e+00, -1.0377e+00, -1.8179e-01,\n",
       "          5.5528e-01, -6.4110e-01,  1.2999e+00, -1.1972e+00,  1.1734e-01,\n",
       "          2.0637e+00, -5.2048e-01, -6.0122e-01, -1.7046e-01, -7.3891e-01,\n",
       "          5.3379e-01, -8.5820e-01, -1.9682e-01,  8.1326e-02,  4.4848e-01,\n",
       "         -3.4697e-01, -1.2831e+00,  4.5158e-01, -3.4600e-01,  1.0332e+00],\n",
       "        [ 1.5352e-01,  1.5614e-01,  4.7000e-01,  1.6596e+00, -3.1631e-01,\n",
       "         -1.3699e+00, -5.5741e-01, -2.6025e+00, -1.0582e+00,  2.4824e+00,\n",
       "          1.0491e+00,  2.6440e+00,  2.5763e-02, -5.8619e-01, -6.9039e-01,\n",
       "          1.7144e-01, -1.0899e+00, -2.4614e-02, -2.2542e-01,  1.4864e+00,\n",
       "          5.9968e-01, -1.6147e+00,  1.6890e-01,  5.2201e-02,  1.0287e+00,\n",
       "          6.6015e-01, -8.0698e-01,  2.7264e-02, -1.6179e+00, -5.1237e-01,\n",
       "         -5.7325e-01, -3.3494e-01,  1.1906e+00,  1.8490e-01,  1.2785e+00,\n",
       "          4.1506e-01, -1.5870e+00,  4.2883e-01, -3.9253e-01,  1.4666e+00,\n",
       "          1.7921e+00, -7.9304e-01,  7.0402e-01,  1.3201e+00,  1.2103e+00,\n",
       "         -2.3423e-01,  4.6243e-01,  4.9850e-01, -9.4650e-01,  1.4818e+00,\n",
       "          2.9404e-01,  3.3056e-01,  7.9973e-01,  1.7742e-01, -7.7840e-02,\n",
       "          7.7079e-01,  2.2104e-01,  1.5405e-03,  7.7430e-01,  1.1606e-01,\n",
       "          1.2477e-01,  2.1595e-01,  1.3143e+00, -1.1441e+00,  2.0033e-01,\n",
       "          2.1910e+00, -9.6030e-01, -1.7629e-01, -1.0643e+00, -2.8522e-01,\n",
       "          1.6263e+00, -4.2548e-01, -2.6005e-02,  1.5094e+00,  1.6497e+00,\n",
       "          1.6673e+00, -9.1932e-01, -1.3391e+00, -1.3499e+00, -2.0677e-01,\n",
       "         -6.7201e-01,  2.1453e+00, -8.9304e-01, -6.4803e-01,  6.1248e-01,\n",
       "         -1.1167e-01,  8.5262e-01, -9.4365e-02, -8.5652e-02,  8.7412e-01,\n",
       "          8.2353e-01, -3.2318e-01, -2.1368e+00, -6.4395e-01, -2.1948e-01,\n",
       "         -1.5255e+00,  7.5022e-01,  4.5786e-01,  2.1336e+00,  5.5340e-01,\n",
       "          4.4300e-01, -1.1928e+00,  1.2893e-01, -4.3988e-02,  4.2616e-01,\n",
       "          5.9797e-01, -1.2159e+00, -8.1042e-01, -2.6346e-01,  5.1969e-01,\n",
       "          1.6775e+00, -1.2392e+00,  1.3752e+00,  6.0837e-01,  3.3549e-01,\n",
       "         -4.2884e-01,  5.4013e-01, -1.9832e+00,  5.4610e-02,  1.1871e+00,\n",
       "          2.2153e-01,  1.6085e+00,  1.3136e+00, -7.2670e-01,  2.6368e-02,\n",
       "          1.8933e+00,  6.5538e-01,  1.6230e-01,  1.1778e-01,  2.2821e-01,\n",
       "         -3.6353e-01,  1.3767e+00, -2.1052e+00, -2.8788e-01,  5.1569e-01,\n",
       "          1.3624e+00, -2.9925e-01,  4.1086e-02,  4.2625e-01, -5.1685e-01,\n",
       "         -2.0077e-01,  7.5239e-01,  8.8370e-01, -5.5722e-02,  9.6339e-01,\n",
       "         -8.8489e-01,  4.6355e-01, -1.1889e+00,  1.8912e-01, -5.6403e-02,\n",
       "          5.1522e-01, -1.2552e+00,  5.8485e-01,  2.8034e-01,  3.2814e-01,\n",
       "          6.4162e-01, -3.4230e-01, -2.1240e+00, -3.1591e-01,  2.5044e-01,\n",
       "          1.4003e+00, -5.5680e-01, -1.2243e+00,  7.2701e-01, -1.0749e+00,\n",
       "          1.0921e+00,  1.1538e+00, -6.0569e-01, -7.6091e-01, -2.1533e-01,\n",
       "          3.0731e-01, -2.1127e+00, -9.9992e-04, -6.9133e-02,  1.5064e+00,\n",
       "         -1.3853e+00,  2.9088e-01,  2.9572e-01, -4.9437e-01, -1.4483e+00,\n",
       "         -1.3503e+00,  1.0378e-01, -1.1863e+00, -1.1476e-01,  3.2725e-01,\n",
       "         -8.3657e-02, -2.4105e-01,  5.9116e-02, -3.2800e-01, -4.4605e-01,\n",
       "          3.9801e-01, -5.2163e-01,  2.0032e-01, -1.2497e+00,  5.2702e-01,\n",
       "         -5.8904e-01,  4.2707e-01, -1.3856e-01,  7.7116e-02, -5.6753e-01,\n",
       "          5.2515e-01,  7.9770e-01, -6.4093e-01,  3.5526e-01, -4.5521e-01,\n",
       "          8.0562e-01, -1.2188e+00,  1.4870e-01, -1.1466e+00,  1.5649e-01,\n",
       "          1.6579e+00,  2.8044e-02, -2.6204e-01,  8.4904e-01, -1.2764e-02,\n",
       "         -7.2426e-01, -7.8688e-01,  4.5422e-01,  8.7687e-01, -2.3981e-01,\n",
       "          4.8102e-01, -2.0028e-01,  9.4704e-01, -2.7501e-01, -3.1639e-01,\n",
       "          2.5624e-01,  1.4353e+00, -9.5870e-01, -2.3228e+00,  9.3142e-02,\n",
       "          9.7697e-02,  7.2185e-01,  5.5794e-01, -9.8011e-01,  1.6618e-01,\n",
       "         -4.2477e-01,  1.4543e+00, -1.5981e+00, -7.0410e-04,  1.8227e+00,\n",
       "          1.8235e+00, -8.8972e-01,  1.0844e+00, -3.7167e-01,  1.0534e+00,\n",
       "          7.1991e-01, -4.4841e-01,  3.7397e-01,  1.2055e+00,  1.1864e-01,\n",
       "          1.2084e-01,  6.7370e-01,  6.2648e-01, -1.8572e+00, -1.0879e+00,\n",
       "          4.0124e-02, -2.1742e+00, -8.4491e-01,  5.7578e-01,  8.8187e-01,\n",
       "         -2.6978e-01, -1.3474e+00,  8.6460e-01,  3.3847e-02, -5.9174e-01,\n",
       "         -2.4836e-01,  4.1848e-01, -1.9168e+00,  6.1103e-01,  4.1982e-02,\n",
       "          7.2644e-01,  1.9036e-01,  2.8670e-02, -3.6787e-01,  5.5563e-02,\n",
       "         -6.3346e-01,  1.0275e+00,  1.0601e+00, -1.1818e+00, -1.4315e-01,\n",
       "          1.2308e+00, -1.7244e+00, -3.9503e-01, -7.1104e-01,  1.3903e+00,\n",
       "         -6.0806e-02,  4.2428e-01, -8.5088e-01, -2.6401e+00, -1.1301e+00,\n",
       "          1.4863e-01,  7.1564e-01, -3.6153e-01, -6.4475e-01,  1.7472e+00,\n",
       "          9.6237e-01,  7.0567e-01,  6.2272e-01, -1.4283e+00,  1.8637e+00,\n",
       "         -5.7345e-01,  2.2494e+00,  1.0546e+00,  2.1777e+00,  2.8675e-01,\n",
       "          7.2475e-01, -5.4552e-01,  1.3633e+00,  5.4309e-01, -1.3276e-01,\n",
       "         -9.9415e-02,  7.6301e-01, -1.5547e+00, -4.7316e-01,  1.0515e+00,\n",
       "         -1.6741e+00,  9.0242e-01, -1.3611e+00, -1.3801e+00,  1.0910e-01,\n",
       "          1.8303e-01,  1.2097e-01,  2.6271e+00,  6.2874e-01, -6.6365e-01,\n",
       "         -1.7075e-01,  1.2716e+00,  2.7775e-01, -1.5802e+00, -1.4027e+00,\n",
       "          5.9321e-01, -6.2425e-01, -1.4775e+00, -2.4795e+00,  4.6451e-01,\n",
       "          1.9772e+00, -1.9959e+00, -1.7062e-01,  8.0834e-01, -2.8669e-01,\n",
       "          2.7547e-01, -1.4943e+00, -6.8901e-01,  1.4582e+00, -1.9932e+00,\n",
       "          4.0776e-01, -4.1027e-01, -6.0256e-02, -7.1906e-01, -9.7859e-01,\n",
       "          3.2277e-02,  2.6142e-01, -1.0973e+00,  1.0348e+00,  1.7291e+00,\n",
       "         -6.3548e-01, -3.1003e-01, -4.6884e-01, -3.1579e-02,  8.0667e-01,\n",
       "         -5.7675e-01,  1.1258e+00,  1.4178e+00,  5.4096e-01,  1.5845e-01,\n",
       "         -7.1688e-01, -1.5393e-01, -7.7314e-02,  1.3823e+00, -4.8283e-01,\n",
       "          7.7278e-01,  1.0753e-01,  4.7648e-01, -1.7288e+00, -5.7626e-01,\n",
       "         -3.8337e-01, -1.7329e+00,  3.3961e-03,  1.3952e-01, -5.0828e-01,\n",
       "          5.7313e-01,  1.6079e+00,  1.5522e+00,  6.6060e-01, -1.8706e-01,\n",
       "         -6.6861e-01,  2.0408e+00, -7.3321e-01,  2.0757e-01, -1.3775e+00,\n",
       "          1.4231e+00,  1.0896e+00, -9.0569e-02, -5.3850e-01, -1.8701e+00,\n",
       "          9.3264e-01,  6.7874e-01,  1.5067e+00, -6.5821e-01,  5.3555e-01,\n",
       "         -1.3650e+00,  9.7098e-02, -1.1602e+00, -3.6921e-01, -7.7821e-01,\n",
       "          1.4518e-01, -9.3974e-01, -2.6455e-01, -6.0044e-01, -9.0854e-01,\n",
       "         -1.6831e+00, -9.0702e-01,  5.5923e-01, -8.2566e-01, -5.8378e-02,\n",
       "         -5.8520e-01,  1.5211e+00, -6.5994e-01,  3.5635e-01, -1.0931e+00,\n",
       "         -7.0262e-02,  2.2963e-01, -4.7937e-01, -1.0809e+00, -4.3393e-01,\n",
       "          7.7693e-01,  2.8857e-01, -5.5095e-01,  3.2592e-01, -6.5687e-01,\n",
       "         -1.2161e+00, -1.8553e-01,  8.5443e-01,  3.8157e-03, -4.6974e-01,\n",
       "          3.4293e-01,  3.8347e-01,  2.1106e+00, -2.3284e-01, -1.0068e+00,\n",
       "          6.0304e-01, -2.5706e-01, -1.6212e+00,  1.3654e+00, -7.9766e-01,\n",
       "          1.0847e+00,  7.7700e-01,  3.3122e-01, -2.3416e+00, -5.1066e-01,\n",
       "         -1.2146e-01, -1.2530e+00, -1.4207e-01,  1.1400e-01,  1.9269e+00,\n",
       "          9.9835e-01,  2.2755e+00,  4.9695e-01,  7.4895e-01,  3.4044e-01,\n",
       "         -4.8410e-01,  6.2293e-01,  2.7862e-01, -3.3803e-01,  3.1603e-01,\n",
       "         -1.5304e-01,  1.5487e-03,  1.0814e+00, -2.1646e+00, -6.7783e-01,\n",
       "          1.0749e+00,  1.4815e+00, -6.4864e-01,  1.5005e-01, -9.8716e-01,\n",
       "          3.2447e-01,  1.0604e+00,  9.5454e-01, -3.0847e-01, -3.9298e-01,\n",
       "          1.8574e-02,  2.9719e-01,  1.1383e-01,  8.1986e-01, -3.3006e-01,\n",
       "         -2.3687e-01, -5.4436e-01,  3.3304e-01,  7.6917e-01,  1.0205e-01,\n",
       "         -1.0231e+00, -5.2323e-01, -1.1632e+00, -8.8728e-01, -3.5255e-01,\n",
       "         -1.3345e-01,  5.8085e-01, -1.5765e+00,  4.4911e-01,  1.0012e+00],\n",
       "        [-5.3405e-02,  2.9949e-01,  1.2848e+00, -1.2305e+00,  2.7866e-01,\n",
       "         -1.3602e+00,  9.3523e-01,  9.4214e-01, -1.1252e+00,  1.1390e+00,\n",
       "         -1.1078e+00, -2.2166e-02,  8.6557e-01,  7.7014e-01,  6.1976e-01,\n",
       "         -1.6376e+00,  9.4238e-01, -1.6674e+00,  1.0144e+00,  1.6714e+00,\n",
       "          7.3140e-01,  1.1561e+00, -2.4560e+00,  6.3744e-01, -2.5225e-01,\n",
       "         -3.7238e-01, -1.7038e-01,  1.6964e+00,  4.1742e-01,  6.1808e-01,\n",
       "          2.7223e-01,  1.0188e+00,  2.2813e+00, -4.7087e-02, -1.2199e+00,\n",
       "          2.3896e+00, -4.2919e-01,  1.2849e-03,  7.4347e-01,  2.3014e+00,\n",
       "         -5.8775e-01,  1.1962e-01, -1.6607e+00,  9.6603e-01, -2.6087e-01,\n",
       "          1.6455e+00, -1.5810e+00, -1.5233e+00,  3.5272e-01,  4.0690e-01,\n",
       "         -4.0500e-01,  7.5723e-02,  9.9800e-01,  1.5081e+00,  8.3229e-01,\n",
       "         -1.8605e-01, -4.1153e-01,  4.5587e-01,  9.7879e-01, -1.0705e+00,\n",
       "         -1.4528e-01, -4.4306e-01,  4.0804e-01,  3.3690e-01,  2.1603e-01,\n",
       "         -6.0451e-01,  4.9019e-01,  1.0926e+00,  1.6494e+00, -1.4704e-01,\n",
       "          1.2451e+00, -1.6599e+00, -3.0137e-01, -4.7116e-01,  9.1710e-02,\n",
       "         -8.3152e-01,  7.9155e-01,  6.1694e-01, -1.1536e+00, -5.8844e-01,\n",
       "         -2.1092e-01, -4.8993e-01, -1.8360e+00, -4.1531e-01,  7.8466e-01,\n",
       "          1.2764e-01,  1.8576e+00, -1.4422e+00, -6.9354e-01,  7.8531e-01,\n",
       "          6.0569e-04, -3.9277e-01, -1.4920e+00,  1.4500e-01, -6.9349e-01,\n",
       "         -5.5822e-02,  5.7928e-01,  1.1900e+00, -1.8382e+00,  6.2645e-01,\n",
       "         -9.1984e-01,  4.6158e-01, -8.7309e-01,  6.6610e-01,  3.7216e-01,\n",
       "          4.4284e-01, -6.9251e-01, -1.2485e+00, -1.3681e+00,  6.5953e-01,\n",
       "         -2.6307e-01,  7.0903e-01,  8.2853e-01, -3.2589e-01, -8.5387e-01,\n",
       "         -3.2722e-01, -6.9153e-01,  9.5822e-01, -1.2468e-01,  6.4001e-01,\n",
       "         -3.2577e-01,  3.4619e-01,  4.1822e-01, -1.7098e-01, -1.0787e+00,\n",
       "         -1.2672e+00, -1.4421e-01, -9.6262e-03, -1.3905e+00,  1.7970e-01,\n",
       "          1.4053e+00, -9.0721e-01,  1.7260e-01,  1.3146e+00,  1.3704e+00,\n",
       "         -3.2646e-02, -3.1223e-01, -1.0207e-01, -2.0698e+00, -4.5195e-01,\n",
       "          2.0213e-01, -4.1483e-01,  1.2953e+00, -2.7461e+00,  8.5035e-01,\n",
       "         -3.8535e-01,  3.5005e-01, -1.2163e-01, -1.8971e+00,  2.3653e+00,\n",
       "          1.7339e-01,  1.5791e+00, -4.5031e-01,  6.3744e-01, -2.3933e-01,\n",
       "          7.1912e-01,  2.5033e-01, -7.5026e-01,  3.5792e-02,  1.8209e+00,\n",
       "         -1.3351e+00,  6.4462e-02, -5.2224e-01, -2.3297e-01, -3.2800e-01,\n",
       "         -1.0922e+00, -1.9056e-02,  2.7128e-01,  2.1328e+00, -6.9746e-01,\n",
       "         -5.0580e-01, -9.4064e-02,  9.3496e-01, -4.3533e-01, -6.6842e-01,\n",
       "          2.6288e+00, -6.5342e-01,  2.8179e-01, -6.6539e-01, -3.5950e-01,\n",
       "          8.4052e-01,  8.8405e-01, -1.4711e+00, -8.4825e-01,  1.1335e+00,\n",
       "          6.8714e-01,  1.8663e+00, -3.4109e-01, -2.0779e-01,  8.9362e-01,\n",
       "         -2.2398e-01, -6.4693e-01,  9.0931e-02, -3.1626e-01,  4.1178e-01,\n",
       "          7.4462e-01,  1.6820e+00,  3.9267e-01,  2.4801e-02,  1.2644e+00,\n",
       "         -5.7893e-01, -1.1549e+00, -4.6868e-01,  6.7378e-01,  5.1600e-01,\n",
       "         -6.7380e-01,  1.5658e+00, -1.3842e+00, -4.2642e-01,  1.1807e-02,\n",
       "         -1.3196e+00,  3.0548e-01, -9.0867e-01,  2.9157e-01, -7.8461e-01,\n",
       "         -4.8066e-01, -3.8963e-01,  4.7757e-02, -2.8291e+00,  2.1736e-01,\n",
       "         -3.8065e-01,  1.9768e+00,  4.6429e-01,  7.6724e-01,  2.2852e-01,\n",
       "          6.9767e-01,  1.8061e+00,  9.6701e-01,  1.1676e+00, -5.9623e-01,\n",
       "         -1.3108e+00, -9.5865e-01,  1.8101e-02,  1.5210e+00,  8.3629e-01,\n",
       "         -1.1717e+00, -1.7058e+00, -1.7775e+00,  2.9645e-01, -1.1323e+00,\n",
       "          1.6211e-01, -5.0120e-01, -1.0250e+00,  1.1153e-01, -5.2913e-02,\n",
       "         -1.1164e+00, -1.9752e+00,  7.1899e-01, -2.8559e-01, -2.8990e-01,\n",
       "         -4.5634e-01, -3.5878e-01, -7.3152e-01,  9.3062e-01,  1.1153e-01,\n",
       "          1.9583e+00, -2.0450e-01, -4.2428e-01,  2.9049e-02, -1.1227e-01,\n",
       "          2.4242e-02, -6.8009e-01, -1.4741e-01, -7.4739e-01,  8.8512e-02,\n",
       "         -2.3829e-01,  3.8827e-01,  3.2786e-02,  9.6222e-01,  9.3703e-01,\n",
       "          4.2159e-01,  1.1411e+00,  5.6490e-01, -1.6222e+00,  1.9444e-01,\n",
       "         -1.1322e+00,  1.3306e+00,  7.3460e-01,  7.5978e-02,  3.2466e-01,\n",
       "         -9.6704e-01, -2.5615e+00,  2.0159e+00, -1.4341e+00,  9.9449e-01,\n",
       "         -1.1447e+00, -4.1536e-01, -3.5779e-01, -1.2984e+00, -1.2462e+00,\n",
       "          1.5592e+00,  1.1369e+00,  1.3789e+00, -3.5517e-01,  7.5213e-03,\n",
       "         -1.3498e+00,  1.5610e-01, -1.3928e+00,  1.0351e+00,  2.0526e-02,\n",
       "         -6.7356e-01, -4.2009e-01, -1.5626e+00,  1.4293e+00, -3.9212e-01,\n",
       "          1.3434e+00, -9.2841e-01, -5.6715e-01, -9.2600e-01, -2.1163e-01,\n",
       "          3.2502e-01, -1.0905e-01,  6.8453e-02, -1.2297e-01, -4.2894e-01,\n",
       "         -6.6411e-01,  7.5819e-01,  3.1417e-01,  1.2988e+00,  5.2263e-01,\n",
       "          1.8103e-01,  1.1308e+00, -9.3675e-01,  2.4094e-01, -1.1503e+00,\n",
       "         -1.1505e+00,  3.6946e-01, -4.6397e-01,  8.6264e-01,  9.1277e-01,\n",
       "         -9.8060e-01, -1.1895e-01,  1.2731e+00, -1.5784e-01,  1.0084e+00,\n",
       "         -1.7146e+00,  6.4153e-01, -4.3786e-01, -1.3888e+00, -1.6882e-01,\n",
       "          5.7432e-01, -6.8915e-02, -4.4133e-01,  1.1304e+00, -2.3614e-01,\n",
       "          1.0223e+00, -1.2282e+00, -1.4098e+00,  1.7168e+00, -7.4829e-01,\n",
       "         -1.2935e+00,  4.0684e-01, -3.0646e-01,  1.4132e+00,  6.2787e-01,\n",
       "         -1.3599e-01, -1.1976e-01, -9.5932e-01,  1.6548e+00, -3.1880e-01,\n",
       "         -4.9834e-01, -1.0015e+00, -1.0122e+00,  4.9544e-01,  2.3373e-01,\n",
       "          1.4694e-01,  5.7787e-01,  4.7666e-01, -1.7091e-01,  1.5514e+00,\n",
       "          1.9713e-02, -3.5772e-01,  1.4446e-01, -3.4115e-01, -1.6149e-01,\n",
       "          1.0018e+00,  1.4767e+00, -9.6933e-01, -1.3108e+00, -1.1439e+00,\n",
       "          5.3196e-01,  1.5066e+00,  5.5400e-01,  5.1941e-01, -7.0749e-01,\n",
       "          3.6139e-01, -5.9093e-01,  1.1290e+00, -6.8954e-01,  1.3873e+00,\n",
       "          6.8916e-01, -2.9042e-01,  1.9321e+00, -2.3179e+00,  2.1222e+00,\n",
       "          2.5028e-02, -5.6617e-01, -1.0486e-01,  6.9003e-01,  2.8900e-01,\n",
       "         -4.7747e-01, -1.7019e+00, -2.4401e-01,  4.6670e-01,  1.5384e+00,\n",
       "         -4.1132e-01, -1.6677e-01,  1.4086e+00,  1.1697e+00, -4.9237e-01,\n",
       "         -5.7252e-01,  4.0650e-01, -1.1525e+00,  5.9098e-02,  5.4852e-01,\n",
       "         -7.5610e-01, -3.0195e-01, -2.0084e+00,  4.8173e-01,  6.0178e-01,\n",
       "          2.0296e+00,  6.1545e-01, -1.6097e+00, -9.4341e-01,  6.9918e-01,\n",
       "         -7.8751e-01,  1.0401e+00, -2.5746e+00, -1.7371e-02, -9.9501e-01,\n",
       "         -7.4157e-01, -4.1182e-01, -6.4841e-01, -1.2779e+00,  1.3774e+00,\n",
       "          7.3298e-01, -8.9639e-01,  2.7982e+00, -1.2246e-01, -1.1934e+00,\n",
       "          1.1824e-01,  5.5905e-01, -1.9375e+00,  6.3918e-01,  4.8270e-01,\n",
       "         -5.8842e-01, -4.0977e-01,  4.4994e+00, -7.6838e-01, -8.4218e-02,\n",
       "         -1.9169e-01,  2.5443e-01, -1.5052e+00,  3.1080e-02, -1.3857e+00,\n",
       "          3.8403e-01, -2.6993e-01, -7.1152e-01,  5.3473e-01, -1.0820e+00,\n",
       "          5.3159e-01,  5.1925e-02,  1.2125e+00,  2.2156e-01, -1.1256e-01,\n",
       "          8.4459e-01,  2.5376e+00, -1.7811e+00,  5.5588e-01, -2.1794e+00,\n",
       "         -7.9733e-01,  1.3886e+00,  2.0152e+00, -2.0596e+00,  1.1805e-01,\n",
       "          1.5238e+00, -7.2982e-01,  4.8426e-01,  3.2555e-01, -9.5493e-02,\n",
       "         -1.3401e-01, -9.3318e-01, -8.8497e-01, -8.3511e-01, -3.0219e-01,\n",
       "         -1.3909e+00, -8.0722e-01,  5.1339e-01, -5.8823e-02, -1.1568e+00,\n",
       "         -1.7586e-01,  1.6158e-01,  1.4904e+00, -2.0833e-02, -5.5169e-01,\n",
       "         -1.1813e+00, -2.8254e-01,  1.6641e+00,  5.7487e-02, -7.2505e-01],\n",
       "        [ 1.9464e-01,  1.9747e+00,  6.4749e-01,  1.8699e-01,  4.4275e-01,\n",
       "         -6.4217e-01,  1.2581e+00,  1.0435e+00,  6.0694e-01,  1.2925e+00,\n",
       "         -1.1761e-01, -1.7026e+00, -2.3276e+00,  3.2283e-01, -1.1350e+00,\n",
       "          1.8000e+00, -1.1588e+00, -2.4182e-01,  2.1374e+00,  1.2116e+00,\n",
       "         -9.8850e-01, -4.5414e-01,  1.4653e+00, -2.9446e-01,  9.0851e-01,\n",
       "         -8.7499e-02, -2.4018e+00, -2.8213e-01,  9.9322e-02,  1.9502e+00,\n",
       "         -1.9372e-03,  3.0004e+00, -1.1492e+00,  7.7902e-01, -2.9379e-01,\n",
       "          4.0172e-01, -1.8988e+00,  6.5088e-01,  1.2921e+00,  1.0405e+00,\n",
       "         -2.8582e-01,  4.2303e-01, -2.1072e+00, -3.2728e-01, -5.2725e-01,\n",
       "         -1.8822e+00, -7.9418e-01,  2.0733e-01, -1.5814e-01, -2.9121e-01,\n",
       "         -8.1293e-01,  8.8238e-01, -1.4896e+00,  1.3100e-01,  1.9558e+00,\n",
       "         -6.9561e-03, -1.1274e+00, -7.9901e-01, -1.2893e+00, -2.4285e-01,\n",
       "          1.3220e-01,  1.3410e+00,  4.4518e-01, -1.4679e+00, -2.6354e+00,\n",
       "          1.6684e+00,  1.5851e-01, -3.8687e-01,  1.0998e-01,  2.6513e-01,\n",
       "          7.3740e-01, -9.1882e-01,  1.4554e+00,  8.7501e-01,  9.2837e-01,\n",
       "          1.9251e+00,  1.0541e+00,  3.9219e-01, -4.0032e-01,  3.3209e-01,\n",
       "         -8.5533e-01,  1.0205e-01, -1.2475e+00,  2.6728e-01, -1.0737e-01,\n",
       "          2.3479e-01,  3.9806e-01,  7.1789e-02, -6.0996e-01,  1.5693e+00,\n",
       "          1.0092e+00, -1.4838e+00, -1.2870e+00, -4.2001e-01,  1.8336e+00,\n",
       "          2.8966e-01,  3.9282e-01,  1.2642e+00,  9.0543e-01,  3.1723e-01,\n",
       "          8.1710e-01, -7.1142e-01,  5.5391e-01,  2.6424e-01,  3.9971e-02,\n",
       "          9.8923e-01, -1.1143e+00, -1.6699e+00,  6.1054e-01,  8.1343e-01,\n",
       "         -6.8536e-01, -2.6370e-01,  1.8085e+00,  1.4640e-01,  6.7711e-01,\n",
       "          6.2623e-02,  2.8480e-02, -5.2230e-01, -1.2897e+00,  4.9100e-01,\n",
       "         -7.7645e-01,  1.3631e+00,  5.2739e-01, -1.0116e+00,  1.5042e+00,\n",
       "         -4.2303e-01, -2.0245e+00,  8.6460e-01,  6.8111e-01, -2.4215e+00,\n",
       "          6.3369e-01,  6.4366e-01,  4.5224e-01, -3.1933e-01,  4.4843e-01,\n",
       "         -6.8261e-01, -4.2309e-02,  6.2118e-01, -8.1736e-01, -2.9217e+00,\n",
       "         -3.1207e-01,  2.3678e+00, -5.2330e-01,  1.7321e-02, -3.2968e-01,\n",
       "          8.3521e-01,  1.9492e+00, -4.1179e-01,  5.0984e-01, -2.2090e-01,\n",
       "          8.9529e-01, -1.8684e+00,  1.9377e-01,  1.2086e+00,  6.3260e-01,\n",
       "          9.9482e-01,  1.4718e+00,  1.3390e-01,  2.5376e+00, -1.5384e-01,\n",
       "         -3.2360e-01, -2.0924e+00, -1.2956e+00,  1.6916e+00, -1.5195e+00,\n",
       "          5.6256e-01, -1.3620e+00,  1.4114e+00,  2.5181e-01, -1.2481e-01,\n",
       "         -9.3101e-01,  1.2629e+00,  8.0317e-02, -1.1163e+00,  1.4218e+00,\n",
       "         -5.8142e-01, -5.9731e-01,  9.8927e-01,  5.4391e-01, -1.9148e+00,\n",
       "          1.9487e+00,  1.1358e+00,  8.2490e-01, -3.1592e-01,  9.2184e-01,\n",
       "         -2.7998e-01,  2.1164e-01, -7.3220e-02,  1.4789e+00, -2.9961e-01,\n",
       "          8.4080e-01,  7.9243e-01, -1.0016e+00,  3.6274e-01, -6.3291e-01,\n",
       "          9.4048e-01,  1.8081e+00, -8.5964e-01,  1.1401e+00,  2.1674e+00,\n",
       "         -1.4422e+00,  1.4563e+00,  2.7292e-02, -6.7679e-01, -1.7209e+00,\n",
       "         -6.9888e-01, -9.2294e-01, -6.8719e-01, -1.1365e+00,  1.0769e+00,\n",
       "          1.0708e+00,  4.0666e-02,  1.0423e+00, -9.9793e-02, -3.4096e-01,\n",
       "          6.6003e-01, -4.6979e-01,  1.2702e+00, -5.1319e-01,  1.2005e+00,\n",
       "          9.1391e-02,  7.7080e-02, -1.7565e+00,  4.8387e-01, -1.2958e+00,\n",
       "         -1.3022e+00,  3.6718e-01,  3.2613e-01, -8.6902e-01, -1.0940e+00,\n",
       "          4.8463e-01, -7.8914e-01,  8.2290e-01,  5.3137e-01, -5.4968e-01,\n",
       "          1.4180e+00,  1.4558e+00,  1.0699e-01, -5.6916e-01,  8.4348e-01,\n",
       "         -1.3492e+00,  1.8031e+00, -7.9348e-01, -1.3500e-01,  1.1882e-01,\n",
       "         -6.0579e-01,  7.8413e-01,  4.0247e-01, -4.0279e-01,  6.1857e-01,\n",
       "          1.2373e-01, -4.2150e-01,  9.3224e-01, -8.9295e-01, -1.7621e-03,\n",
       "         -9.7283e-01, -7.4780e-01, -7.3133e-01, -3.5193e-01,  4.4210e-01,\n",
       "         -1.3017e+00,  9.5678e-01, -3.3952e-01, -1.4670e-01,  2.9268e-01,\n",
       "         -5.3548e-01,  2.7600e-01, -1.2527e-01, -5.8458e-01, -1.4430e+00,\n",
       "          1.0588e+00,  4.4149e-01, -6.1051e-01, -8.6793e-02, -1.4722e+00,\n",
       "         -1.5027e+00,  1.7214e+00, -5.2669e-01, -1.6205e+00, -1.7036e+00,\n",
       "         -1.4553e+00,  1.6263e-01, -1.5526e+00,  1.2197e+00,  2.0094e-01,\n",
       "         -1.7199e+00,  3.4965e-01,  3.6830e-01, -1.9264e+00,  5.2903e-01,\n",
       "         -9.5592e-01,  1.1891e+00,  1.2233e-01,  3.9237e-01, -1.8323e+00,\n",
       "         -5.9589e-01,  1.8556e+00,  1.4372e+00,  9.0135e-01,  5.8923e-01,\n",
       "         -2.5289e-02,  1.0802e+00,  2.1577e-01, -5.3046e-01, -1.3685e+00,\n",
       "          7.4030e-01, -1.5195e+00, -1.1858e+00, -3.1817e-01,  5.0839e-01,\n",
       "          8.5684e-02,  4.6719e-01, -1.2072e-01, -6.4673e-01,  1.1259e+00,\n",
       "         -7.8586e-01,  6.5219e-01, -1.1279e+00, -5.3851e-01,  1.6636e-01,\n",
       "         -1.1484e+00, -1.3630e+00,  1.9202e+00,  7.5928e-01, -8.3254e-01,\n",
       "         -1.4622e+00,  4.1637e-01, -7.9486e-01, -2.8893e-01,  7.4427e-02,\n",
       "         -4.1981e-02,  5.9100e-01, -1.6503e+00, -1.0104e+00,  8.2447e-01,\n",
       "         -9.2596e-01,  6.5322e-01,  7.4271e-02,  1.4906e+00,  3.9546e-01,\n",
       "         -7.5914e-02, -6.0927e-01,  2.2457e-01,  2.6547e-01,  1.5518e+00,\n",
       "         -1.3131e-01,  1.1385e+00,  2.1902e+00, -4.1777e-01, -2.1900e+00,\n",
       "         -4.6973e-01, -4.1925e-01, -1.9566e+00, -3.0545e-01,  1.1342e+00,\n",
       "         -5.3903e-01,  1.1881e+00,  5.5909e-01,  2.7950e-01,  4.3915e-01,\n",
       "          1.6743e+00, -1.8108e-01,  6.8216e-01,  1.3215e-01,  8.6824e-01,\n",
       "         -1.2976e+00,  1.7277e-01,  3.5848e-01,  7.9043e-01,  1.5102e-01,\n",
       "         -4.5849e-01, -1.3577e+00,  1.8857e-01, -4.4885e-03,  6.3981e-01,\n",
       "          1.4073e+00, -1.7967e+00, -6.7238e-01,  1.3698e+00, -4.6329e-01,\n",
       "         -1.2331e+00, -3.7266e-01,  5.2220e-01,  2.2534e-01, -6.3906e-01,\n",
       "          1.5392e+00,  1.2160e-01,  1.5809e+00,  1.4656e+00,  1.1847e+00,\n",
       "          2.4898e-01,  1.5923e+00, -4.3528e-01, -4.5236e-01, -1.6963e-01,\n",
       "          1.2421e+00,  4.1852e-01,  1.3262e+00, -1.0354e+00,  1.0775e+00,\n",
       "          6.0322e-01,  1.0141e+00, -2.4382e-01,  5.5250e-01, -9.0251e-01,\n",
       "          1.3326e-01, -1.5123e+00, -1.1922e+00,  1.7722e+00,  9.2923e-01,\n",
       "         -1.1958e+00,  4.2734e-02,  1.2662e+00,  3.3232e-01, -3.0107e-01,\n",
       "          1.2370e+00, -6.2095e-02,  1.0009e+00,  3.6462e-01,  4.9029e-01,\n",
       "          1.8019e+00,  1.2283e+00,  1.6605e+00,  8.8647e-01, -1.4424e-01,\n",
       "          3.5644e-01, -1.6502e+00,  5.3847e-01,  1.1290e+00, -8.4097e-01,\n",
       "          6.0118e-01, -4.2111e-01,  1.6844e-02, -1.2283e+00, -1.6698e-01,\n",
       "          1.7906e+00,  1.0499e+00, -1.8959e-01,  1.2334e+00, -8.9827e-02,\n",
       "          7.3145e-01,  4.2190e-01,  1.2134e+00,  1.2365e+00, -1.0280e+00,\n",
       "         -2.7530e-01, -2.7701e-01, -1.2581e+00,  4.7195e-01, -7.8480e-01,\n",
       "         -7.1151e-01,  9.4991e-01, -1.9217e-01,  7.1201e-03,  1.0213e+00,\n",
       "          7.7086e-01, -1.7846e+00,  3.1106e+00, -6.9254e-01, -2.4807e+00,\n",
       "          1.7391e+00, -1.4833e-01, -1.8206e+00, -3.5273e-01, -1.9261e-01,\n",
       "          5.8558e-01,  4.0736e-02,  1.4801e+00,  8.1488e-01, -4.8764e-01,\n",
       "         -6.1690e-01,  5.2945e-01, -1.2826e+00, -3.9732e-01, -8.7966e-01,\n",
       "          2.0482e+00,  1.0373e+00,  7.5613e-01, -6.4876e-01,  1.1956e+00,\n",
       "          1.5337e+00, -9.1231e-01,  9.9330e-01, -1.1316e-01, -2.2769e+00,\n",
       "          2.6474e-01, -9.3592e-01,  1.3525e+00,  1.7430e+00, -4.4181e-01,\n",
       "          1.7622e-01, -1.6853e+00,  3.0584e-01, -4.8399e-01, -1.4841e+00,\n",
       "          8.8265e-02,  1.1157e+00,  6.7729e-01,  1.8914e-01,  4.6012e-01]])"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence, H3K4me3, H3K36me2, H3K27me3, H3K9me3, methylation, coordinates = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 500])\n",
      "torch.Size([8, 1, 500])\n"
     ]
    }
   ],
   "source": [
    "print(H3K4me3.shape)\n",
    "print(H3K4me3.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0,  ..., 0, 0, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 0],\n",
       "         [0, 0, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 1,  ..., 1, 0, 1],\n",
       "         [0, 1, 0,  ..., 0, 1, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 1, 1],\n",
       "         [0, 1, 1,  ..., 1, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 1, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 1],\n",
       "         [0, 0, 0,  ..., 1, 0, 0]],\n",
       "\n",
       "        [[1, 0, 1,  ..., 1, 0, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 1, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 1],\n",
       "         [1, 0, 0,  ..., 1, 1, 0]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0].shape    # [8, 500, 4] I need to reshape this to (B,C=4,L=500)\n",
    "seq[0].permute(0,2, 1)    # [8, 500, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 0],\n",
       "        [0, 0, 1,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0][0].transpose(-2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 0],\n",
       "        [0, 0, 1,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0].permute(0, 2, 1)[0]#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m----> 2\u001b[0m             nn\u001b[38;5;241m.\u001b[39mConv1d(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDNA_layer1_kernel_height, \u001b[38;5;241m4\u001b[39m), \n\u001b[1;32m      3\u001b[0m                         stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDNA_layer1_stride, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDNA_layer1_padding)\n\u001b[1;32m      4\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "[\n",
    "    [[1,\n",
    "      0,\n",
    "      0,\n",
    "      1]\n",
    "     ],\n",
    "     [[0,0,1,0],\n",
    "      [0,0,0,0],\n",
    "      [0,0,0,0],\n",
    "      [0,0,1,0]],\n",
    "     [],\n",
    "     [],CHANNEL4], BATCH1\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    " ]\n",
    " BATCH=5, CHANNEL=4, WIDTH=, HEIGHT="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_Conv1D(length, kernel_size,\n",
    "                  padding, stride, dilation=1):\n",
    "    return np.floor((length+2*padding-dilation*(kernel_size-1)-1)/stride+1).astype(int)\n",
    "\n",
    "def get_out_MaxPool1D(length, kernel_size,\n",
    "                  padding, stride, dilation=1):\n",
    "    return np.floor((length+2*padding-dilation*(kernel_size-1)-1)/stride+1).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "k1=10\n",
    "s1=2\n",
    "\n",
    "k2=10\n",
    "s2=5\n",
    "\n",
    "k3=5\n",
    "s3=3\n",
    "\n",
    "k4=5\n",
    "s4=3\n",
    "\n",
    "size1 = get_out_Conv1D(length=500, kernel_size=k1, padding=0, stride=s1)\n",
    "# size2 = get_out_MaxPool1D(length=size1, kernel_size=k2 ,padding=0, stride=s2, dilation=1)\n",
    "size3 = get_out_Conv1D(length=size1, kernel_size=k3, padding=0, stride=s3)\n",
    "size4 = get_out_MaxPool1D(length=size3, kernel_size=k4, padding=0, stride=s4, dilation=1)\n",
    "\n",
    "\n",
    "print(size4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 18)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2, h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPyOpt check this library (== Optuna)\n",
    "\n",
    "### Model from \n",
    "\n",
    "# def d_cnn_model(input_length):\n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Dropout(0.2, input_shape=(input_length,1)))\n",
    "#     model.add(Conv1D(32, 3, activation='relu'))\n",
    "#     # model.add(Conv1D(32, 3, activation='relu'))\n",
    "#     # model.add(Dropout(0.5))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "\n",
    "#     model.add(Conv1D(64, 3, activation='relu'))\n",
    "#     # # model.add(Dropout(0.5))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "\n",
    "#     model.add(Conv1D(128, 3, activation='relu'))\n",
    "#     # # model.add(Dropout(0.5))\n",
    "#     model.add(MaxPooling1D(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
